"""
ëª¨ë¸ì´ SDO ì´ë¯¸ì§€ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. ì •ìƒ ì…ë ¥ìœ¼ë¡œ ì˜ˆì¸¡
2. SDO ì´ë¯¸ì§€ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡
3. ë‘ ì˜ˆì¸¡ ê°„ ì°¨ì´ í™•ì¸

ì°¨ì´ê°€ ê±°ì˜ ì—†ë‹¤ë©´ â†’ ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
"""

import torch
import numpy as np
from pathlib import Path
import hydra
from omegaconf import DictConfig

from networks import create_model
from pipeline import create_dataloader


def load_trained_model(checkpoint_path: str, config: DictConfig, device: str = 'cuda'):
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    model = create_model(config)
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    elif 'state_dict' in checkpoint:
        model.load_state_dict(checkpoint['state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model = model.to(device)
    model.eval()
    
    return model


@hydra.main(config_path="./configs", config_name="config", version_base=None)
def main(config: DictConfig):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    checkpoint_path = "/opt/projects/10_Harim/01_AP/04_Result/model_epoch0100.pth"
    device = "cpu"  # CPUë¡œ í™•ì‹¤í•˜ê²Œ
    
    print("=" * 70)
    print("MODEL IMAGE USAGE VERIFICATION")
    print("=" * 70)
    print(f"\nCheckpoint: {checkpoint_path}")
    print(f"Device: {device}\n")
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_trained_model(checkpoint_path, config, device)
    print("âœ“ Model loaded\n")
    
    # ë°ì´í„° ë¡œë“œ
    dataloader = create_dataloader(config, phase="validation")
    
    # ì—¬ëŸ¬ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    NUM_SAMPLES = 10
    
    results = []
    
    for idx, batch in enumerate(dataloader):
        if idx >= NUM_SAMPLES:
            break
        
        solar_wind = batch["inputs"][:1].to(device)
        images = batch["sdo"][:1].to(device)
        
        with torch.no_grad():
            # 1. ì •ìƒ ì˜ˆì¸¡
            pred_normal = model(solar_wind, images)
            
            # 2. ì´ë¯¸ì§€ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì˜ˆì¸¡
            images_zero = torch.zeros_like(images)
            pred_no_image = model(solar_wind, images_zero)
            
            # 3. ì´ë¯¸ì§€ë¥¼ ëœë¤ ë…¸ì´ì¦ˆë¡œ ë°”ê¿”ì„œ ì˜ˆì¸¡
            images_noise = torch.randn_like(images)
            pred_noise = model(solar_wind, images_noise)
        
        # ì°¨ì´ ê³„ì‚°
        diff_zero = torch.abs(pred_normal - pred_no_image).mean().item()
        diff_noise = torch.abs(pred_normal - pred_noise).mean().item()
        
        results.append({
            'sample': idx,
            'pred_normal': pred_normal[0, 0, 0].item(),
            'pred_no_image': pred_no_image[0, 0, 0].item(),
            'pred_noise': pred_noise[0, 0, 0].item(),
            'diff_zero': diff_zero,
            'diff_noise': diff_noise
        })
        
        print(f"Sample {idx:2d}: "
              f"Normal={pred_normal[0,0,0].item():6.3f}, "
              f"NoImage={pred_no_image[0,0,0].item():6.3f}, "
              f"Noise={pred_noise[0,0,0].item():6.3f}, "
              f"Diff_zero={diff_zero:.6f}, "
              f"Diff_noise={diff_noise:.6f}")
    
    # í†µê³„
    print("\n" + "=" * 70)
    print("STATISTICS")
    print("=" * 70)
    
    diffs_zero = [r['diff_zero'] for r in results]
    diffs_noise = [r['diff_noise'] for r in results]
    
    mean_diff_zero = np.mean(diffs_zero)
    max_diff_zero = np.max(diffs_zero)
    
    mean_diff_noise = np.mean(diffs_noise)
    max_diff_noise = np.max(diffs_noise)
    
    print(f"\nWhen replacing images with ZEROS:")
    print(f"  Mean prediction difference: {mean_diff_zero:.6f}")
    print(f"  Max prediction difference:  {max_diff_zero:.6f}")
    
    print(f"\nWhen replacing images with NOISE:")
    print(f"  Mean prediction difference: {mean_diff_noise:.6f}")
    print(f"  Max prediction difference:  {max_diff_noise:.6f}")
    
    # ì§„ë‹¨
    print("\n" + "=" * 70)
    print("DIAGNOSIS")
    print("=" * 70)
    
    THRESHOLD = 0.01  # 1% ë³€í™”
    
    if mean_diff_zero < THRESHOLD:
        print("\nğŸ”´ CRITICAL ISSUE DETECTED!")
        print(f"   Prediction changes by only {mean_diff_zero:.6f} when images are removed.")
        print("   â†’ Model is NOT using SDO images effectively!")
        print("\n   Possible reasons:")
        print("   1. ConvLSTM features have very small magnitude")
        print("   2. Cross-modal fusion heavily favors transformer features")
        print("   3. Model learned to rely only on OMNI data")
        print("   4. ConvLSTM weights are near zero / not trained properly")
        
        print("\n   Recommended actions:")
        print("   a) Check training logs - did ConvLSTM loss decrease?")
        print("   b) Examine cross-modal fusion weights")
        print("   c) Try training with higher weight on image loss")
        print("   d) Validate that SDO images have signal (not all zeros)")
        
    elif mean_diff_zero < 0.1:
        print("\nâš ï¸  WARNING: Low image importance")
        print(f"   Prediction changes by {mean_diff_zero:.6f} when images are removed.")
        print("   â†’ Model uses images, but they have minimal impact.")
        print("   â†’ OMNI data is much more important than SDO images.")
        
    else:
        print("\nâœ… Model is using images!")
        print(f"   Prediction changes by {mean_diff_zero:.6f} when images are removed.")
        print("   â†’ Images contribute meaningfully to predictions.")
    
    # Saliency map ìœ ì˜ë¯¸ì„±
    print("\n" + "=" * 70)
    print("SALIENCY MAP IMPLICATIONS")
    print("=" * 70)
    
    if mean_diff_zero < THRESHOLD:
        print("\nâŒ Saliency maps will NOT be meaningful!")
        print("   Since the model doesn't use images, gradients will be:")
        print("   - Near zero")
        print("   - Uniform across all pixels")
        print("   - Not interpretable")
        print("\n   â†’ Fix the model training first before analyzing saliency!")
        
    else:
        print("\nâœ… Saliency maps should be interpretable.")
        print("   The model uses images, so gradients should show:")
        print("   - Spatial patterns")
        print("   - Temporal variations")
        print("   - Meaningful attributions")
    
    print("\n" + "=" * 70)


if __name__ == '__main__':
    main()
