# Saliency ë¶„ì„ ê²°ê³¼ ë° ê°œì„  ë°©ì•ˆ

## ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½

### âœ… ì •ìƒì ì¸ ë¶€ë¶„

1. **ì±„ë„ ì¤‘ìš”ë„ê°€ ë¬¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•¨**
   ```
   193Ã…:        1.0000  (ê³ ì˜¨ ì½”ë¡œë‚˜)
   211Ã…:        0.9952  (í™œë™ ì˜ì—­)
   magnetogram: 0.8477  (ìê¸°ì¥)
   ```
   - ê³ ì˜¨ ì½”ë¡œë‚˜ê°€ ì§€ìê¸° í™œë™ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”
   - ëª¨ë“  ì±„ë„ì´ ì ì ˆíˆ ê¸°ì—¬ (0.85+)

2. **Grad-CAMì´ íŠ¹ì • ì˜ì—­ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ì£¼ëª©**
   - ì™¼ìª½ ìƒë‹¨ ëŒ€í˜• êµ¬ì¡°
   - ì˜¤ë¥¸ìª½ ì—¬ëŸ¬ í™œë™ ì˜ì—­
   - í•˜ë‹¨ ì¤‘ì•™ êµ¬ì¡°

### ğŸ”´ ì‹¬ê°í•œ ë¬¸ì œì 

#### ë¬¸ì œ 1: ì‹œê°„ ì •ë³´ë¥¼ ì œëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

**ì¦ê±°:**

1. **Grad-CAMì´ ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼**
   ```
   t=0, t=14, t=27ì˜ Grad-CAMì´ ì™„ì „íˆ ë™ì¼
   ì›ë³¸ ì´ë¯¸ì§€ëŠ” ëª…ë°±íˆ ë‹¤ë¥¸ë° saliencyëŠ” ë¶ˆë³€
   ```

2. **ì¤‘ê°„ ì‹œì ì„ ê±°ì˜ ë¬´ì‹œ**
   ```
   Temporal Importance:
   t=0-3:    0.2-0.4  (ì•½ê°„ ì‚¬ìš©)
   t=4-14:   0.0-0.2  (ê±°ì˜ ë¬´ì‹œ!) â† 12ì‹œê°„ ë¶„ëŸ‰
   t=15-19:  0.7-0.75 (ì‚¬ìš©)
   t=20-22:  0.1      (ë¬´ì‹œ)
   t=23-27:  0.8-1.0  (ì£¼ë¡œ ì‚¬ìš©)
   ```

3. **ì˜ˆì¸¡ê°’ì´ ê±°ì˜ ì¼ì •**
   ```
   ëª¨ë“  ì‹œì : -0.2 Â± 0.01
   ```

**ì˜ë¯¸:**
- ConvLSTMì´ **ì‹œê°„ ì •ë³´ë¥¼ í†µí•©í•˜ì§€ ëª»í•¨**
- ë§ˆì¹˜ **ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ì²˜ëŸ¼** ì‘ë™
- ì´ˆê¸° + ìµœê·¼ë§Œ ë³´ê³  ì¤‘ê°„ ì§„í™” ê³¼ì • ë¬´ì‹œ

#### ë¬¸ì œ 2: ê³µê°„ì  íŒ¨í„´ì´ ê³ ì •ë¨

**ì¦ê±°:**
- ì›ë³¸ì´ íšŒì „í•˜ê³  ë³€í•´ë„ saliencyëŠ” ë¶ˆë³€
- íŠ¹ì • ìœ„ì¹˜ë§Œ í•­ìƒ ì£¼ëª©

**ì˜ë¯¸:**
- ìœ„ì¹˜ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± í•™ìŠµ
- "ì™¼ìª½ ìƒë‹¨ì´ í•­ìƒ ì¤‘ìš”í•˜ë‹¤" ê°™ì€ ë‹¨ìˆœ ê·œì¹™
- ì‹¤ì œ íƒœì–‘ ë¬¼ë¦¬ í˜„ìƒê³¼ ë¬´ê´€

#### ë¬¸ì œ 3: ëª¨ë¸ ì˜ˆì¸¡ì´ ì…ë ¥ê³¼ ë…ë¦½ì 

**ì¦ê±°:**
- ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ -0.2 ì¶œë ¥
- verify_image_usage ê²°ê³¼ì™€ ì¼ì¹˜

**ì˜ë¯¸:**
- í•™ìŠµ ì‹¤íŒ¨
- í‰ê· ê°’ë§Œ ì¶œë ¥

---

## ğŸ” ê·¼ë³¸ ì›ì¸ ì§„ë‹¨

### ê°€ì„¤ 1: ConvLSTM Hidden State ë¯¸ì „ë‹¬

**ê°€ëŠ¥ì„±: 70%**

```python
# ì˜ˆìƒë˜ëŠ” ë¬¸ì œ
for t in range(seq_len):
    frame = images[:, :, t, :, :]
    output, hidden = convlstm(frame, hidden)
    # â† hiddenì´ ì œëŒ€ë¡œ ì—…ë°ì´íŠ¸/ì „ë‹¬ë˜ì§€ ì•ŠìŒ
```

**ê²°ê³¼:**
- ê° í”„ë ˆì„ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
- ì‹œê°„ì  ë§¥ë½ ì†ì‹¤

### ê°€ì„¤ 2: Feature Magnitude Imbalance

**ê°€ëŠ¥ì„±: 80%**

```python
# ì˜ˆìƒë˜ëŠ” ìƒí™©
transformer_features: mean = 10.0
convlstm_features:    mean = 0.01  â† 100ë°° ì‘ìŒ!

# Fusion
fused = transformer_feat + convlstm_feat
      â‰ˆ transformer_feat  (convlstm ë¬´ì‹œë¨)
```

**ê²°ê³¼:**
- Cross-modal fusionì—ì„œ ConvLSTM ë¬´ì‹œ
- Transformerë§Œìœ¼ë¡œ ì˜ˆì¸¡

### ê°€ì„¤ 3: Gradient Vanishing

**ê°€ëŠ¥ì„±: 50%**

```python
# Long sequence (28 steps) â†’ vanishing gradients
# ì´ˆê¸° ì‹œì ì— gradientê°€ ë„ë‹¬í•˜ì§€ ëª»í•¨
```

**ê²°ê³¼:**
- ConvLSTMì´ í•™ìŠµë˜ì§€ ì•ŠìŒ
- ì´ˆê¸° ê°€ì¤‘ì¹˜ ìƒíƒœ ìœ ì§€

---

## ğŸ’¡ í•´ê²° ë°©ì•ˆ

### ğŸ¯ Phase 1: ì§„ë‹¨ (ì¦‰ì‹œ ì‹¤í–‰)

#### Step 1: ConvLSTM ì‘ë™ í™•ì¸

```bash
python diagnose_convlstm.py --config-name saliency
```

**í™•ì¸ ì‚¬í•­:**
- [ ] Hidden stateê°€ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ”ê°€?
- [ ] ê° ì‹œì ì˜ outputì´ ë‹¤ë¥¸ê°€?
- [ ] Gradientê°€ ì—­ì „íŒŒë˜ëŠ”ê°€?

#### Step 2: Fusion ê°€ì¤‘ì¹˜ í™•ì¸

```bash
python diagnose_fusion.py --config-name saliency
```

**í™•ì¸ ì‚¬í•­:**
- [ ] Transformerì™€ ConvLSTMì˜ feature í¬ê¸° ë¹„ìœ¨
- [ ] Fusionì—ì„œ ê° modalityì˜ ê¸°ì—¬ë„
- [ ] Ablation test ê²°ê³¼

---

### ğŸ”§ Phase 2: ë¹ ë¥¸ ìˆ˜ì • (1-2ì¼)

#### ìˆ˜ì • 1: Feature Normalization ì¶”ê°€

**ë¬¸ì œ:** ConvLSTM featuresê°€ ë„ˆë¬´ ì‘ìŒ

**í•´ê²°:**
```python
class CrossModalFusion(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # ì¶”ê°€!
        self.transformer_norm = nn.LayerNorm(transformer_dim)
        self.convlstm_norm = nn.LayerNorm(convlstm_dim)
        
    def forward(self, transformer_feat, convlstm_feat):
        # Normalize!
        transformer_feat = self.transformer_norm(transformer_feat)
        convlstm_feat = self.convlstm_norm(convlstm_feat)
        
        # Then fuse
        fused = self.fusion_layer(torch.cat([transformer_feat, convlstm_feat], dim=-1))
        return fused
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ë‘ modalityì˜ í¬ê¸° ê· í˜•
- ConvLSTMì˜ ê¸°ì—¬ë„ ì¦ê°€

#### ìˆ˜ì • 2: ConvLSTM Learning Rate ì¦ê°€

**ë¬¸ì œ:** ConvLSTMì´ í•™ìŠµë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```python
# Separate optimizer groups
optimizer = optim.AdamW([
    {'params': model.transformer_model.parameters(), 'lr': 1e-4},
    {'params': model.convlstm_model.parameters(), 'lr': 5e-4},  # 5ë°°!
    {'params': model.cross_modal_fusion.parameters(), 'lr': 1e-4},
    {'params': model.regression_head.parameters(), 'lr': 1e-4}
])
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ConvLSTM ê°€ì¤‘ì¹˜ ë¹ ë¥¸ ì—…ë°ì´íŠ¸
- Transformerì— catch up

#### ìˆ˜ì • 3: Gradient Clipping ì¡°ì •

**ë¬¸ì œ:** Gradient vanishing/exploding

**í•´ê²°:**
```python
# ë” ê´€ëŒ€í•œ clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # 1.0 â†’ 5.0

# ë˜ëŠ” per-module clipping
torch.nn.utils.clip_grad_norm_(model.convlstm_model.parameters(), max_norm=10.0)
```

---

### ğŸ—ï¸ Phase 3: êµ¬ì¡° ê°œì„  (1ì£¼)

#### ê°œì„  1: Auxiliary Loss ì¶”ê°€

**ëª©ì :** ConvLSTMì´ ì˜ë¯¸ ìˆëŠ” featuresë¥¼ í•™ìŠµí•˜ë„ë¡ ê°•ì œ

```python
class MultiModalModel(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # ... existing modules ...
        
        # Auxiliary head for ConvLSTM
        self.convlstm_auxiliary_head = nn.Sequential(
            nn.Linear(convlstm_dim, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )
    
    def forward(self, solar_wind, images):
        transformer_feat = self.transformer_model(solar_wind)
        convlstm_feat = self.convlstm_model(images)
        
        # Auxiliary prediction (only during training)
        if self.training:
            aux_pred = self.convlstm_auxiliary_head(convlstm_feat)
        
        # Main prediction
        fused = self.cross_modal_fusion(transformer_feat, convlstm_feat)
        main_pred = self.regression_head(fused)
        
        if self.training:
            return main_pred, aux_pred
        else:
            return main_pred

# Loss
main_loss = criterion(main_pred, target)
aux_loss = criterion(aux_pred, target)
total_loss = main_loss + 0.3 * aux_loss  # 30% weight on auxiliary
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ConvLSTMì´ ì§ì ‘ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë„ë¡ í•™ìŠµ
- ì˜ë¯¸ ìˆëŠ” visual features ì¶”ì¶œ

#### ê°œì„  2: Attention-based Fusion

**ëª©ì :** ë™ì ìœ¼ë¡œ modality ê°€ì¤‘ì¹˜ ê²°ì •

```python
class AttentionFusion(nn.Module):
    def __init__(self, transformer_dim, convlstm_dim, hidden_dim):
        super().__init__()
        
        # Query, Key, Value projections
        self.q_transformer = nn.Linear(transformer_dim, hidden_dim)
        self.k_convlstm = nn.Linear(convlstm_dim, hidden_dim)
        self.v_transformer = nn.Linear(transformer_dim, hidden_dim)
        self.v_convlstm = nn.Linear(convlstm_dim, hidden_dim)
        
        self.scale = hidden_dim ** -0.5
        self.output = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, transformer_feat, convlstm_feat):
        # Cross-attention
        q_t = self.q_transformer(transformer_feat)
        k_c = self.k_convlstm(convlstm_feat)
        
        # Attention weights
        attn = torch.softmax(q_t @ k_c.T * self.scale, dim=-1)
        
        # Weighted combination
        v_t = self.v_transformer(transformer_feat)
        v_c = self.v_convlstm(convlstm_feat)
        
        fused_t = attn @ v_c
        fused_c = attn.T @ v_t
        
        # Concatenate and project
        fused = self.output(torch.cat([fused_t, fused_c], dim=-1))
        
        return fused
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ì…ë ¥ì— ë”°ë¼ adaptiveí•˜ê²Œ fusion
- ê° modalityì˜ ê¸°ì—¬ë„ ê´€ì°° ê°€ëŠ¥

#### ê°œì„  3: Temporal Attention in ConvLSTM

**ëª©ì :** ì‹œê°„ ì •ë³´ë¥¼ ë” íš¨ê³¼ì ìœ¼ë¡œ í™œìš©

```python
class ConvLSTMWithAttention(nn.Module):
    def __init__(self, ...):
        super().__init__()
        self.convlstm = ConvLSTM(...)
        
        # Temporal attention
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4
        )
        
    def forward(self, images):
        # images: (B, C, T, H, W)
        B, C, T, H, W = images.shape
        
        # ConvLSTM
        features = []
        for t in range(T):
            feat = self.convlstm(images[:, :, t, :, :])  # (B, hidden_dim)
            features.append(feat)
        
        features = torch.stack(features, dim=0)  # (T, B, hidden_dim)
        
        # Temporal attention
        attn_out, attn_weights = self.temporal_attention(
            features, features, features
        )
        
        # Aggregate
        output = attn_out.mean(dim=0)  # (B, hidden_dim)
        
        return output
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ì¤‘ìš”í•œ ì‹œì ì— ì§‘ì¤‘
- ì „ì²´ ì‹œí€€ìŠ¤ í™œìš©

---

### ğŸ§ª Phase 4: ì¬í•™ìŠµ ì „ëµ (2ì£¼)

#### ì „ëµ 1: Progressive Training

**Stage 1 (Epoch 1-20): ConvLSTMë§Œ í•™ìŠµ**
```python
# Freeze other modules
for param in model.transformer_model.parameters():
    param.requires_grad = False
for param in model.cross_modal_fusion.parameters():
    param.requires_grad = False

# Train only ConvLSTM
optimizer = optim.AdamW(model.convlstm_model.parameters(), lr=1e-3)
```

**Stage 2 (Epoch 21-50): Fusion í•™ìŠµ**
```python
# Unfreeze fusion
for param in model.cross_modal_fusion.parameters():
    param.requires_grad = True

optimizer = optim.AdamW([
    {'params': model.convlstm_model.parameters(), 'lr': 5e-4},
    {'params': model.cross_modal_fusion.parameters(), 'lr': 1e-3}
])
```

**Stage 3 (Epoch 51-100): Fine-tuning**
```python
# Unfreeze all
optimizer = optim.AdamW(model.parameters(), lr=1e-4)
```

#### ì „ëµ 2: Curriculum Learning

**Easy â†’ Hard**
```python
# Stage 1: Short sequences (T=7)
dataloader = create_dataloader(config, seq_len=7)

# Stage 2: Medium sequences (T=14)
dataloader = create_dataloader(config, seq_len=14)

# Stage 3: Full sequences (T=28)
dataloader = create_dataloader(config, seq_len=28)
```

**ê¸°ëŒ€ íš¨ê³¼:**
- LSTMì´ gradientsë¥¼ ë” ì˜ ì „íŒŒ
- ì ì§„ì  í•™ìŠµ

#### ì „ëµ 3: Data Augmentation

**ì‹œê°„ì  augmentation:**
```python
def temporal_augmentation(images):
    # Random temporal shift
    shift = random.randint(-2, 2)
    images = torch.roll(images, shifts=shift, dims=2)
    
    # Random temporal flip
    if random.random() > 0.5:
        images = torch.flip(images, dims=[2])
    
    # Random frame dropout
    if random.random() > 0.5:
        mask = torch.rand(images.shape[2]) > 0.1
        images = images[:, :, mask, :, :]
    
    return images
```

**ê¸°ëŒ€ íš¨ê³¼:**
- ë” robustí•œ ì‹œê°„ ì •ë³´ í•™ìŠµ
- Overfitting ë°©ì§€

---

## ğŸ“‹ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1: ì§„ë‹¨

- [ ] Day 1: `diagnose_convlstm.py` ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„
- [ ] Day 2: `diagnose_fusion.py` ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„
- [ ] Day 3: ë¬¸ì œì  ì •ë¦¬ ë° í•´ê²° ë°©ì•ˆ ìš°ì„ ìˆœìœ„ ê²°ì •

### Week 2: ë¹ ë¥¸ ìˆ˜ì •

- [ ] Day 1-2: Feature normalization ì¶”ê°€
- [ ] Day 3-4: Learning rate ì¡°ì • ë° ì¬í•™ìŠµ ì‹œì‘
- [ ] Day 5: ì¤‘ê°„ ê²°ê³¼ í™•ì¸ (saliency ì¬ìƒì„±)

### Week 3: êµ¬ì¡° ê°œì„ 

- [ ] Day 1-2: Auxiliary loss êµ¬í˜„
- [ ] Day 3-4: Attention fusion êµ¬í˜„
- [ ] Day 5: ì„±ëŠ¥ ë¹„êµ

### Week 4: ì¬í•™ìŠµ ë° í‰ê°€

- [ ] Day 1-3: Progressive training
- [ ] Day 4: ìµœì¢… saliency ë¶„ì„
- [ ] Day 5: ë…¼ë¬¸ figure ì‘ì„±

---

## ğŸ¯ ì˜ˆìƒ ê²°ê³¼

### ì„±ê³µ ì‹œ (Good Case)

**Temporal Importance:**
```
ë¶€ë“œëŸ¬ìš´ ê³¡ì„ , ì—¬ëŸ¬ ì‹œì ì— ë¶„ì‚°
ëª¨ë“  ì‹œì ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒ ê¸°ì—¬
```

**Grad-CAM:**
```
ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” íŒ¨í„´
ì›ë³¸ ì´ë¯¸ì§€ì˜ êµ¬ì¡° ë³€í™”ë¥¼ ë°˜ì˜
```

**ì˜ˆì¸¡:**
```
ì…ë ¥ì— ë”°ë¼ ë³€í•˜ëŠ” ì˜ˆì¸¡ê°’
ë” ë†’ì€ ì •í™•ë„
```

### ì¤‘ê°„ ì„±ê³¼ (Moderate)

**Temporal Importance:**
```
ì¼ë¶€ ì‹œì ì— ì§‘ì¤‘
í•˜ì§€ë§Œ ì—¬ì „íˆ ì˜ë¯¸ ìˆëŠ” ë¶„í¬
```

**Grad-CAM:**
```
ì•½ê°„ì˜ ì‹œê°„ì  ë³€í™”
ì£¼ìš” êµ¬ì¡°ëŠ” ì¶”ì 
```

### ì‹¤íŒ¨ ì‹œ (Bad Case)

**ì—¬ì „íˆ ë™ì¼í•œ íŒ¨í„´:**
```
â†’ ë” ê·¼ë³¸ì ì¸ ë¬¸ì œ (ì•„í‚¤í…ì²˜ ì¬ì„¤ê³„ í•„ìš”)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

**Papers:**
1. "Attention is All You Need" - Multi-head attention
2. "ConvLSTM" - Spatial-temporal modeling
3. "Grad-CAM++" - Improved saliency
4. "Progressive Neural Networks" - Transfer learning

**Debugging:**
1. Hidden state visualization
2. Gradient magnitude tracking
3. Feature distribution analysis
4. Ablation studies

---

**ì‘ì„±ì¼**: 2025-01-12  
**ìƒíƒœ**: ì§„ë‹¨ ì™„ë£Œ, ê°œì„  ëŒ€ê¸°  
**ìš°ì„ ìˆœìœ„**: ğŸ”´ High (ëª¨ë¸ ì„±ëŠ¥ì— critical)
