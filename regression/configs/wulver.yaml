# Enhanced Multi-Modal Model Configuration
# Configuration for transformer + ConvLSTM + CrossModalFusion architecture

defaults:
  - _self_
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none

hydra:
  output_subdir: null
  run:
    dir: .
  job:
    chdir: false

experiment:
  experiment_name: 'wulver'
  seed: 250104
  batch_size: 4
  enable_undersampling: true
  num_subsample: 14
  subsample_index: 0
  input_days:
   - 1
   - 2
   - 3
   - 4
   - 5
   - 6
   - 7
  target_days:
    - 1
    - 2
    - 3

environment:
  device: 'cuda'
  data_root: '/mmfs1/home/hl545/ap/renew/datasets'
  save_root: '/mmfs1/home/hl545/ap/renew/results'
  num_workers: 2

data:
  data_dir_name: 'oversampling_13'
  dataset_name: 'three_twelve_to_one'

  sdo_wavelengths:
    - 'aia_193'
    - 'aia_211'
    - 'hmi_magnetogram'
  sdo_image_size: 64
  sdo_start_index: 12
  sdo_end_index: 40

  input_variables:
      - 'bx_gse_gsm_nt'
      - 'by_gsm_nt'
      - 'bz_gsm_nt'
      - 'b_magnitude_of_avg_field_vector_nt'
      - 'plasma_flow_speed_km_s'
      - 'proton_density_n_cm3'
      - 'proton_temperature_k'
      - 'kp_index'
      - 'ap_index_nt'
      - 'dst_index_nt'
      - 'f10_7_index_sfu'
      - 'sunspot_number_r'
  input_start_index: 24
  input_end_index: 80

  target_variables:
    - 'ap_index_nt'
  target_start_index: 80
  target_end_index: 104

model:
  # Model type: "fusion" (default), "transformer", "convlstm"
  model_type: 'fusion'

  # Transformer configuration
  transformer_d_model: 256
  transformer_nhead: 8
  transformer_num_layers: 3
  transformer_dim_feedforward: 512
  transformer_dropout: 0.1

  # ConvLSTM configuration
  convlstm_input_channels: 3
  convlstm_hidden_channels: 64
  convlstm_kernel_size: 3
  convlstm_num_layers: 2

  # Cross-modal fusion configuration
  fusion_num_heads: 4
  fusion_dropout: 0.1

training:
  regression_loss_type: 'mse'
  contrastive_loss_type: 'consistency'  # 'consistency' or 'infonce'
  infonce_temperature: 0.3  # Only used if contrastive_type='infonce'
  lambda_contrastive: 0.1  # Weight for contrastive loss

  optimizer: 'adam'  # 'adam' or 'sgd'
  learning_rate: 0.0002

  num_epochs: 100
  report_freq: 100
  model_save_freq: 20

validation:
  checkpoint_path: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/checkpoint/model_epoch0100.pth'
  output_dir: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/validation/epoch_0100'
  compute_alignment: true
  save_plots: true

mcd:
  checkpoint_path: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/checkpoint/model_epoch0100.pth'
  output_dir: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/mcd/epoch_0100'

saliency:
  checkpoint_path: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/checkpoint/model_epoch0100.pth'
  output_dir: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/saliency/epoch_0100'
  n_steps: 30
  target_variable: 0

attention:
  checkpoint_path: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/checkpoint/model_epoch0100.pth'
  output_dir: '/mmfs1/home/hl545/ap/renew/results/${experiment.experiment_name}/attention/epoch_0100'
  create_plots: false
