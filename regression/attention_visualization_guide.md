# Attention Visualization ì™„ë²½ ê°€ì´ë“œ

Transformerì˜ Attention Patternì„ ì´í•´í•˜ê¸° ìœ„í•œ ìƒì„¸ ì„¤ëª…ì„œ

---

## ğŸ“Š ê°œìš”

ì´ ë¬¸ì„œëŠ” Transformer ëª¨ë¸ì˜ attention weightsë¥¼ ì‹œê°í™”í•œ 3ê°œ í”Œë¡¯ì— ëŒ€í•œ ì™„ë²½í•œ í•´ì„ ê°€ì´ë“œì…ë‹ˆë‹¤.

```python
import matplotlib.pyplot as plt
import numpy as np

data = np.load('attention_batch_0000.npz')
attn = data['attention_weights'][-1].mean(axis=0)  # Last layer, avg over heads

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
# Plot 1: Attention Matrix
# Plot 2: Temporal Importance
# Plot 3: Attention Distribution
```

---

## ğŸ¯ Attention Weightì˜ ìˆ˜í•™ì  ì˜ë¯¸

### **1. í™•ë¥  ë¶„í¬ (Probability Distribution)**

```python
# Attention ê³„ì‚° ê³¼ì • (Transformer ë‚´ë¶€)

# 1. Queryì™€ Keyì˜ dot product
scores = Q @ K.T / sqrt(d_k)  # (seq_len, seq_len)

# 2. Softmax â†’ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜
attention_weights = softmax(scores, dim=-1)  # (seq_len, seq_len)

# 3. Valueì— weighted sum ì ìš©
output = attention_weights @ V
```

**í•µì‹¬:**
```python
# ê° rowì˜ í•©ì´ 1.0
attention_weights.sum(axis=-1)  # [1.0, 1.0, 1.0, ..., 1.0]
```

### **Attention weight = 0.2ì˜ ì˜ë¯¸:**

> **"í•´ë‹¹ timestepì´ ì „ì²´ ì •ë³´ì˜ 20%ë¥¼ ê¸°ì—¬í•œë‹¤"**

**êµ¬ì²´ì  ì˜ˆì‹œ:**
```python
# Attention matrixì˜ row 5 (query timestep 5)
attn[5, :] = [0.05, 0.05, 0.10, 0.15, 0.15, 0.20, 0.15, 0.15]
                â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘
               t=0   t=1   t=2   t=3   t=4   t=5   t=6   t=7

# í•´ì„:
# Output[t=5] = 0.05 Ã— Input[t=0]  # 5% ê¸°ì—¬
#             + 0.05 Ã— Input[t=1]  # 5% ê¸°ì—¬
#             + 0.10 Ã— Input[t=2]  # 10% ê¸°ì—¬
#             + 0.15 Ã— Input[t=3]  # 15% ê¸°ì—¬
#             + 0.15 Ã— Input[t=4]  # 15% ê¸°ì—¬
#             + 0.20 Ã— Input[t=5]  # 20% ê¸°ì—¬ (ê°€ì¥ ì¤‘ìš”)
#             + 0.15 Ã— Input[t=6]  # 15% ê¸°ì—¬
#             + 0.15 Ã— Input[t=7]  # 15% ê¸°ì—¬
#             = 1.00 (100%)
```

### **ê²€ì¦ ì½”ë“œ:**

```python
import numpy as np

data = np.load('attention_batch_0000.npz')
attn = data['attention_weights'][-1].mean(axis=0)  # (seq_len, seq_len)

# ê° rowì˜ í•© í™•ì¸
row_sums = attn.sum(axis=-1)

print("Row sums (should all be ~1.0):")
print(row_sums)
# [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]

# íŠ¹ì • queryì˜ attention distribution
query_5_attention = attn[5, :]
print(f"\nQuery timestep 5 attention distribution:")
for t, weight in enumerate(query_5_attention):
    print(f"  Key t={t}: {weight:.4f} ({weight*100:.1f}%)")

print(f"\nSum: {query_5_attention.sum():.4f}")
# Sum: 1.0000
```

---

## ğŸ”¥ Plot 1: Attention Matrix (Heatmap)

### **ì½”ë“œ:**
```python
im1 = axes[0].imshow(attn, cmap='hot', aspect='auto')
axes[0].set_xlabel('Key Position (Past)')
axes[0].set_ylabel('Query Position (Current)')
axes[0].set_title('Attention Matrix')
plt.colorbar(im1, ax=axes[0])
```

### **ì¶• ì„¤ëª…:**

| ì¶• | ë²”ìœ„ | ì˜ë¯¸ | ë°©í–¥ |
|----|------|------|------|
| **Xì¶•** | 0 ~ seq_len-1 | Key Position (ì°¸ì¡°ë˜ëŠ” ëŒ€ìƒ) | ê³¼ê±° â†’ í˜„ì¬ |
| **Yì¶•** | 0 ~ seq_len-1 | Query Position (ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ì£¼ì²´) | ê³¼ê±° â†’ í˜„ì¬ |
| **ìƒ‰ìƒ** | 0.0 ~ 0.3 | Attention Weight (ê¸°ì—¬ë„ %) | ì–´ë‘ì›€ â†’ ë°ìŒ |

**Xì¶•: Key Position (ì°¸ì¡°ë˜ëŠ” ëŒ€ìƒ)**
- ì˜ë¯¸: "ì–´ëŠ ì‹œì ì˜ ì •ë³´ë¥¼ ì½ì–´ì˜¬ê¹Œ?"
- ì˜ˆ: X=3 â†’ "Timestep 3ì˜ ë°ì´í„°"

**Yì¶•: Query Position (ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ì£¼ì²´)**
- ì˜ë¯¸: "ì–´ëŠ ì‹œì ì´ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ê°€?"
- ì˜ˆ: Y=5 â†’ "Timestep 5ê°€ ì •ë³´ ìš”ì²­"

**ìƒ‰ìƒ (ê°’): Attention Weight**
- ë°ìŒ (ë…¸ë€ìƒ‰/í°ìƒ‰): ë†’ì€ attention = ì¤‘ìš”í•œ ì—°ê²°
- ì–´ë‘ì›€ (ê²€ì •/ë¹¨ê°•): ë‚®ì€ attention = ì•½í•œ ì—°ê²°

### **ì½ëŠ” ë°©ë²•:**

```python
# Matrixì˜ í•œ ì 
value = attn[5, 3]  # 0.15

# í•´ì„:
# "Query timestep 5ê°€ Key timestep 3ì„ 15% ì°¸ì¡°í•œë‹¤"
# = "í˜„ì¬ ì‹œì  5ê°€ ê³¼ê±° ì‹œì  3ì˜ ì •ë³´ë¥¼ 15% ì‚¬ìš©í•œë‹¤"
```

### **ì£¼ìš” íŒ¨í„´:**

#### **1. Diagonal (ëŒ€ê°ì„ )**
```
attn[0,0], attn[1,1], ..., attn[7,7]
```
- **ì˜ë¯¸**: Self-attention (ìê¸° ìì‹  ì°¸ì¡°)
- **ìƒ‰ìƒ**: ë³´í†µ ë°ìŒ (0.15~0.25)
- **í•´ì„**: "í˜„ì¬ ìƒíƒœê°€ ê°€ì¥ ì¤‘ìš”"

#### **2. Vertical bright line (ì„¸ë¡œ ë°ì€ ì„ )**
```
attn[:, 5]  # ëª¨ë“  queryê°€ Key 5ë¥¼ ì£¼ëª©
```
- **ì˜ë¯¸**: íŠ¹ì • timestepì´ ì „ì²´ì ìœ¼ë¡œ ì¤‘ìš”
- **ìƒ‰ìƒ**: Column ì „ì²´ê°€ ë°ìŒ
- **í•´ì„**: "Timestep 5ì— critical ì •ë³´ ìˆìŒ"

#### **3. Horizontal bright line (ê°€ë¡œ ë°ì€ ì„ )**
```
attn[5, :]  # Query 5ê°€ ì—¬ëŸ¬ keyë¥¼ ì°¸ì¡°
```
- **ì˜ë¯¸**: íŠ¹ì • timestepì´ ë§ì€ ê³¼ê±°ë¥¼ ì°¸ì¡°
- **ìƒ‰ìƒ**: Row ì „ì²´ê°€ ë°ìŒ
- **í•´ì„**: "Timestep 5ëŠ” ë‹¤ì–‘í•œ ì •ë³´ í†µí•©"

### **ì˜ˆì‹œ ë§¤íŠ¸ë¦­ìŠ¤:**

```
Attention Matrix (8Ã—8):
        Key 0   Key 1   Key 2   Key 3   Key 4   Key 5   Key 6   Key 7
Query 0  0.18    0.12    0.10    0.08    0.09    0.20    0.12    0.11
Query 1  0.15    0.20    0.13    0.09    0.08    0.18    0.10    0.07
Query 2  0.12    0.14    0.22    0.11    0.09    0.16    0.09    0.07
Query 3  0.09    0.10    0.14    0.19    0.12    0.20    0.09    0.07
Query 4  0.08    0.09    0.11    0.13    0.21    0.22    0.10    0.06
Query 5  0.05    0.08    0.11    0.09    0.09    0.21    0.16    0.21  â† ì´ row
Query 6  0.07    0.09    0.10    0.11    0.12    0.18    0.20    0.13
Query 7  0.06    0.07    0.09    0.10    0.11    0.16    0.14    0.27
                                                    â†‘
                                              Key 5 column
```

**ì½ê¸°:**
- **Diagonal ë°ìŒ**: Self-attention ê°•í•¨ (ê° timestepì´ ìê¸° ìì‹  ì¤‘ì‹œ)
- **Key 5 column ë°ìŒ**: ëª¨ë“  queryê°€ timestep 5 ì£¼ëª©
- **Query 5 row ë¶„ì‚°**: Timestep 5ëŠ” ì—¬ëŸ¬ ê³¼ê±° ì°¸ì¡°

---

## ğŸ“Š Plot 2: Temporal Importance (Bar Chart)

### **ì½”ë“œ:**
```python
incoming = attn.sum(axis=0)
axes[1].bar(range(input_size), incoming, color='orangered')
axes[1].set_xlabel('Timestep')
axes[1].set_ylabel('Total Incoming Attention')
axes[1].set_title('Temporal Importance')
axes[1].axvline(peak_idx, color='yellow', linestyle='--', label=f'Peak (t={peak_idx})')
```

### **ì¶• ì„¤ëª…:**

| ì¶• | ë²”ìœ„ | ì˜ë¯¸ | ê³„ì‚° |
|----|------|------|------|
| **Xì¶•** | 0 ~ seq_len-1 | Timestep (ì‹œê°„) | ê³¼ê±° â†’ í˜„ì¬ |
| **Yì¶•** | 0.5 ~ 2.0 | Total Incoming Attention | `attn.sum(axis=0)` |

**Xì¶•: Timestep (ì‹œê°„)**
- ì˜ë¯¸: "ê° ì‹œì "
- ì˜ˆ: X=5 â†’ "Timestep 5"

**Yì¶•: Total Incoming Attention**
- ì˜ë¯¸: "í•´ë‹¹ timestepì´ ë°›ëŠ” ì´ attention"
- ê³„ì‚°: `attn.sum(axis=0)` = ê° columnì˜ í•©
- ë†’ì„ìˆ˜ë¡ ë” ë§ì´ ì°¸ì¡°ë¨

### **ê°’ì˜ ì˜ë¯¸:**

```python
# Timestep 5ì˜ incoming attention
incoming[5] = attn[:, 5].sum()
            = attn[0,5] + attn[1,5] + ... + attn[7,5]
            = 0.20 + 0.18 + 0.16 + ... + 0.16
            = 1.48
```

**í•´ì„:**
- **"Timestep 5ëŠ” ì „ì²´ ëª¨ë¸ì—ì„œ 1.48ë§Œí¼ì˜ attentionì„ ë°›ëŠ”ë‹¤"**
- **"ëª¨ë“  queryë“¤ì´ timestep 5ë¥¼ í‰ê·  18.5% ì°¸ì¡°í•œë‹¤" (1.48/8)**

### **ê°’ì˜ ë²”ìœ„:**

```python
# ì´ë¡ ì  ë²”ìœ„
min_possible = 0.0  # ì•„ë¬´ë„ ì°¸ì¡° ì•ˆ í•¨ (ë¶ˆê°€ëŠ¥, softmax ë•Œë¬¸ì—)
max_possible = 8.0  # ëª¨ë“  queryê°€ 100% ì°¸ì¡° (ë¶ˆê°€ëŠ¥, ë¶„ì‚°ë˜ë¯€ë¡œ)

# í˜„ì‹¤ì  ë²”ìœ„
uniform = 1.0  # ê· ë“± ë¶„ì‚° (ê° queryê°€ 1/8ì”© ì°¸ì¡°)
typical_range = [0.5, 2.0]  # ì‹¤ì œ ê´€ì¸¡ë˜ëŠ” ë²”ìœ„
```

### **íŒ¨í„´ í•´ì„:**

```
Incoming Attention:
[0.70, 0.79, 1.00, 0.91, 1.01, 1.48, 1.03, 1.08]
  â†“     â†“     â†“     â†“     â†“     â†‘     â†“     â†“
 ë‚®ìŒ  ë‚®ìŒ  í‰ê·   í‰ê·   í‰ê·   ë†’ìŒ  í‰ê·   í‰ê· 
```

**ì˜ë¯¸:**
- **Timestep 5 (1.48)**: **Critical timestep** - ëª¨ë‘ê°€ ì£¼ëª©
- **Timestep 0-1 (0.70-0.79)**: ëœ ì¤‘ìš” - ì´ˆê¸° ê³¼ë„ê¸°
- **ë‚˜ë¨¸ì§€ (~1.0)**: ë³´í†µ - ê· ë“± ë¶„ì‚°

### **ë¬¼ë¦¬ì  í•´ì„ (íƒœì–‘í’ ì˜ˆì¸¡):**

```python
# ì˜ˆ: input_size=24 (24ì‹œê°„), 12ë¶„ cadence
timesteps_hours = np.arange(24) * 12 / 60  # [0, 0.2, 0.4, ..., 4.6] hours

# Incoming attention í”Œë¡¯
incoming = attn.sum(axis=0)

# Peak at timestep 18
peak_t = incoming.argmax()  # 18
peak_hours = peak_t * 12 / 60  # 3.6 hours ago

print(f"Most important: {peak_hours:.1f} hours before prediction")
# "Most important: 3.6 hours before prediction"
```

**í•´ì„:** "ì˜ˆì¸¡ ì‹œì ìœ¼ë¡œë¶€í„° ì•½ 3.6ì‹œê°„ ì „ì˜ íƒœì–‘í’ ë°ì´í„°ê°€ ê°€ì¥ critical"

---

## ğŸ“ˆ Plot 3: Attention Distribution (Line Plot)

### **ì½”ë“œ:**
```python
axes[2].plot(attn.T, alpha=0.3)  # ê° queryì˜ attention
axes[2].plot(attn.mean(axis=0), 'k-', linewidth=3, label='Average')
axes[2].set_xlabel('Key Position')
axes[2].set_ylabel('Attention Weight')
axes[2].set_title('Attention Distribution')
```

### **ì¶• ì„¤ëª…:**

| ì¶• | ë²”ìœ„ | ì˜ë¯¸ | ì„¤ëª… |
|----|------|------|------|
| **Xì¶•** | 0 ~ seq_len-1 | Key Position (ì°¸ì¡° ëŒ€ìƒ) | ê³¼ê±° â†’ í˜„ì¬ |
| **Yì¶•** | 0.0 ~ 0.3 | Attention Weight (ê°€ì¤‘ì¹˜) | í™•ë¥  (row sum = 1.0) |

**Xì¶•: Key Position (ì°¸ì¡° ëŒ€ìƒ)**
- ì˜ë¯¸: "ì–´ëŠ timestepì„ ì°¸ì¡°í•˜ëŠ”ê°€?"
- ì˜ˆ: X=3 â†’ "Key timestep 3"

**Yì¶•: Attention Weight (ê°€ì¤‘ì¹˜)**
- ì˜ë¯¸: "ê° keyì— ëŒ€í•œ attention ë¹„ì¤‘"
- ë²”ìœ„: 0~1 ì‚¬ì´ (í™•ë¥ )
- ê° lineì˜ Yê°’ í•© = 1.0

### **ì„ ì˜ ì˜ë¯¸:**

#### **1. ì–‡ì€ ì„ ë“¤ (alpha=0.3, ë°˜íˆ¬ëª…)**
```python
for query in range(8):
    axes[2].plot(attn[query, :], alpha=0.3)
```

- **ê° ì„ **: í•˜ë‚˜ì˜ query (timestep)ì˜ attention pattern
- **8ê°œ ì„ **: 8ê°œ queryê°€ ê°ê° ì–´ë–»ê²Œ ê³¼ê±°ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€
- **Yê°’**: í•´ë‹¹ queryê°€ ê° keyì— ì£¼ëŠ” attention

**ì˜ˆì‹œ:**
```python
# Query 5ì˜ line (ë¶„í™ìƒ‰ ë°˜íˆ¬ëª… ì„  í•˜ë‚˜)
line_5 = attn[5, :]  # [0.05, 0.08, 0.11, 0.09, 0.09, 0.21, 0.16, 0.21]

# X=0ì¼ ë•Œ Y=0.05: "Query 5ê°€ Key 0ì„ 5% ì°¸ì¡°"
# X=5ì¼ ë•Œ Y=0.21: "Query 5ê°€ Key 5ë¥¼ 21% ì°¸ì¡°"
```

#### **2. êµµì€ ê²€ì€ ì„  (Average)**
```python
axes[2].plot(attn.mean(axis=0), 'k-', linewidth=3)
```

- **ì˜ë¯¸**: ëª¨ë“  queryì˜ í‰ê·  attention pattern
- **ê³„ì‚°**: `attn.mean(axis=0)` = ê° columnì˜ í‰ê· 
- **Yê°’**: "í‰ê· ì ìœ¼ë¡œ ê° keyê°€ ì–¼ë§ˆë‚˜ ì°¸ì¡°ë˜ëŠ”ê°€?"

```python
# Average line
avg = attn.mean(axis=0)  # (8,)
# = [mean(attn[:, 0]), mean(attn[:, 1]), ..., mean(attn[:, 7])]
# = [0.10, 0.11, 0.12, 0.12, 0.11, 0.19, 0.13, 0.12]
#                                         â†‘
#                                 Key 5ê°€ í‰ê· ì ìœ¼ë¡œ ë†’ìŒ
```

### **íŒ¨í„´ í•´ì„:**

#### **Pattern 1: ì„ ë“¤ì´ ìˆ˜ë ´ (Convergence)**
```
ëª¨ë“  ì–‡ì€ ì„ ë“¤ì´ X=5 ê·¼ì²˜ì—ì„œ ë†’ì•„ì§
â†’ "ëª¨ë“  queryê°€ Key 5ë¥¼ ì¤‘ìš”í•˜ê²Œ ë´„"
â†’ êµµì€ ì„ ë„ X=5ì—ì„œ peak
```

#### **Pattern 2: ì„ ë“¤ì´ ë¶„ì‚° (Divergence)**
```
ì–‡ì€ ì„ ë“¤ì´ ì œê°ê° ë‹¤ë¥¸ pattern
â†’ "ê° queryë§ˆë‹¤ ì°¸ì¡° ì „ëµì´ ë‹¤ë¦„"
â†’ ì¼ë¶€ëŠ” ì´ˆë°˜, ì¼ë¶€ëŠ” í›„ë°˜ ì§‘ì¤‘
```

#### **Pattern 3: Diagonal dominance**
```
ê° ì„ ì´ ìê¸° ìœ„ì¹˜(X=query_idx)ì—ì„œ peak
â†’ Self-attention ê°•í•¨
â†’ í˜„ì¬ ìƒíƒœê°€ ê°€ì¥ ì¤‘ìš”
```

### **ì‹¤ì œ ì˜ˆì‹œ í•´ì„:**

```python
# ë°ì´í„°
attn = np.array([
    [0.18, 0.12, 0.10, 0.08, 0.09, 0.20, 0.12, 0.11],  # Query 0
    [0.15, 0.20, 0.13, 0.09, 0.08, 0.18, 0.10, 0.07],  # Query 1
    [0.12, 0.14, 0.22, 0.11, 0.09, 0.16, 0.09, 0.07],  # Query 2
    [0.09, 0.10, 0.14, 0.19, 0.12, 0.20, 0.09, 0.07],  # Query 3
    [0.08, 0.09, 0.11, 0.13, 0.21, 0.22, 0.10, 0.06],  # Query 4
    [0.05, 0.08, 0.11, 0.09, 0.09, 0.21, 0.16, 0.21],  # Query 5
    [0.07, 0.09, 0.10, 0.11, 0.12, 0.18, 0.20, 0.13],  # Query 6
    [0.06, 0.07, 0.09, 0.10, 0.11, 0.16, 0.14, 0.27],  # Query 7
])

# Plotì— í‘œì‹œë˜ëŠ” ì„ ë“¤
# Line 0 (Query 0): [0.18, 0.12, 0.10, 0.08, 0.09, 0.20, 0.12, 0.11]
#   â†’ X=0ì—ì„œ 0.18, X=5ì—ì„œ 0.20 (peak)
# Line 1 (Query 1): [0.15, 0.20, 0.13, 0.09, 0.08, 0.18, 0.10, 0.07]
#   â†’ X=1ì—ì„œ 0.20 (self), X=5ì—ì„œ 0.18
# ...
# Line 7 (Query 7): [0.06, 0.07, 0.09, 0.10, 0.11, 0.16, 0.14, 0.27]
#   â†’ X=7ì—ì„œ 0.27 (strong self-attention)

# Average line (êµµì€ ê²€ì€ ì„ )
avg = attn.mean(axis=0)  # [0.10, 0.11, 0.12, 0.12, 0.11, 0.19, 0.13, 0.12]
#   â†’ X=5ì—ì„œ 0.19 (peak) - ëª¨ë“  queryê°€ Key 5ë¥¼ í‰ê·  19% ì°¸ì¡°
```

**ì‹œê°ì  í•´ì„:**
```
Y (Attention)
0.3 |
    |           Line 7 (Query 7)
0.2 |    â•±â•²    â•±â•²        â•±
    |   â•±  â•²  â•±  â•²      â•±
0.1 |__â•±____â•²â•±____â•²____â•±_____ Lines 0-6 (ë°˜íˆ¬ëª…)
    |         â†‘          
0.0 |_________________________
    0    2    4    6    8    X (Key Position)
              5 (Peak)
    
    êµµì€ ê²€ì€ ì„  (Average):
             â•±â•²
        ___â•±  â•²___
```

---

## ğŸ¯ ì„¸ í”Œë¡¯ì„ í•¨ê»˜ ì½ê¸°

### **ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤:**

```python
# Plot 1 (Heatmap): Key 5 columnì´ ì „ì²´ì ìœ¼ë¡œ ë°ìŒ
# â†’ "ëª¨ë“  queryê°€ timestep 5ë¥¼ ì£¼ëª©"

# Plot 2 (Bar): Timestep 5ì—ì„œ barê°€ ê°€ì¥ ë†’ìŒ (1.48)
# â†’ "Timestep 5ê°€ í‰ê·  18.5% ì°¸ì¡°ë¨"

# Plot 3 (Line): êµµì€ ê²€ì€ ì„ ì´ X=5ì—ì„œ peak
# â†’ "í‰ê· ì ìœ¼ë¡œ timestep 5ì˜ attentionì´ ë†’ìŒ"
```

**ì¢…í•© í•´ì„:**
> "ëª¨ë¸ì€ **Timestep 5** (ì•½ 1ì‹œê°„ ì „)ì˜ íƒœì–‘í’ ë°ì´í„°ë¥¼ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•œë‹¤"

### **ë¬¼ë¦¬ì  ì˜ë¯¸ (íƒœì–‘í’ ì˜ˆì¸¡):**

| ë°œê²¬ | í”Œë¡¯ | í•´ì„ |
|------|------|------|
| Timestep 5 ì¤‘ìš” | ëª¨ë“  í”Œë¡¯ | ì˜ˆì¸¡ ì „ ~1ì‹œê°„ì˜ ë°ì´í„°ê°€ critical |
| Diagonal ë°ìŒ | Plot 1 | í˜„ì¬ ìƒíƒœë„ ì¤‘ìš” (self-attention) |
| ì´ˆë°˜ ì•½í•¨ | Plot 2 | ë„ˆë¬´ ë¨¼ ê³¼ê±°ëŠ” ëœ ì¤‘ìš” |
| í›„ë°˜ ì¦ê°€ | Plot 2, 3 | ìµœê·¼ ë°ì´í„°ì¼ìˆ˜ë¡ ì¤‘ìš” |

---

## ğŸ“ ìˆ˜í•™ì  ê´€ê³„

### **Plot 2 (Incoming) â†” Plot 3 (Average)**

```python
# Plot 2ì˜ bar ë†’ì´
incoming = attn.sum(axis=0)  # (8,)

# Plot 3ì˜ í‰ê· ì„ 
average = attn.mean(axis=0)  # (8,)

# ê´€ê³„
incoming = average * num_queries
average = incoming / num_queries

# ì˜ˆì‹œ
incoming[5] = 1.48
average[5] = 1.48 / 8 = 0.185

# ì˜ë¯¸: "Timestep 5ëŠ” í‰ê·  18.5% ì°¸ì¡°ë¨"
```

### **Plot 1 (Matrix) â†’ Plot 2 (Bar)**

```python
# Plot 2ëŠ” Plot 1ì˜ column sum
for key in range(8):
    incoming[key] = attn[:, key].sum()
    # = attn[0,key] + attn[1,key] + ... + attn[7,key]
```

### **Plot 1 (Matrix) â†’ Plot 3 (Lines)**

```python
# Plot 3ì˜ ê° ì–‡ì€ ì„  = Plot 1ì˜ ê° row
for query in range(8):
    line = attn[query, :]  # Plot 1ì˜ í•œ row
    plt.plot(line, alpha=0.3)  # Plot 3ì˜ í•œ ì„ 
```

---

## ğŸ’¡ ì‹¤ì „ ë¶„ì„ ì½”ë“œ

### **ì™„ì „í•œ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸:**

```python
import numpy as np
import matplotlib.pyplot as plt

# Load data
data = np.load('attention_batch_0000.npz')
attn = data['attention_weights'][-1].mean(axis=0)  # (seq_len, seq_len)

print("=" * 70)
print("ATTENTION PATTERN ANALYSIS")
print("=" * 70)

# 1. Basic statistics
print(f"\n1. Matrix Shape: {attn.shape}")
print(f"   Min attention: {attn.min():.4f}")
print(f"   Max attention: {attn.max():.4f}")
print(f"   Mean attention: {attn.mean():.4f}")

# 2. Row sums (should be ~1.0)
row_sums = attn.sum(axis=-1)
print(f"\n2. Row Sums (Validation):")
print(f"   All rows sum to 1.0: {np.allclose(row_sums, 1.0)}")
print(f"   Row sums: {row_sums}")

# 3. Temporal importance
incoming = attn.sum(axis=0)
peak_t = incoming.argmax()
print(f"\n3. Temporal Importance:")
print(f"   Peak timestep: {peak_t}")
print(f"   Peak value: {incoming[peak_t]:.4f}")
print(f"   Mean importance: {incoming.mean():.4f}")

# 4. Self-attention strength
diagonal = np.diag(attn)
print(f"\n4. Self-Attention:")
print(f"   Diagonal mean: {diagonal.mean():.4f}")
print(f"   Diagonal strength: {diagonal.sum() / attn.sum():.1%}")

# 5. Attention concentration
entropy = -(attn * np.log(attn + 1e-10)).sum(axis=-1).mean()
print(f"\n5. Attention Concentration:")
print(f"   Entropy: {entropy:.4f} (higher = more uniform)")

# 6. Top-3 connections
flat_indices = np.argsort(attn.flatten())[-3:][::-1]
top_3_queries = flat_indices // attn.shape[1]
top_3_keys = flat_indices % attn.shape[1]
print(f"\n6. Top-3 Strongest Connections:")
for i, (q, k) in enumerate(zip(top_3_queries, top_3_keys)):
    print(f"   {i+1}. Query {q} â†’ Key {k}: {attn[q, k]:.4f}")

# 7. Pattern detection
print(f"\n7. Pattern Detection:")
if diagonal.mean() > 0.15:
    print("   âœ“ Strong self-attention detected")
if incoming[peak_t] > 1.5:
    print(f"   âœ“ Critical timestep detected: t={peak_t}")
if entropy < 2.0:
    print("   âœ“ Focused attention (concentrated pattern)")
elif entropy > 2.5:
    print("   âœ“ Distributed attention (uniform pattern)")
```

### **ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê°ì§€:**

```python
# 1. Uniform attention (ëª¨ë“  ê°’ì´ 0.125)
if attn.std() < 0.01:
    print("âš ï¸  Model is not learning temporal dependencies!")

# 2. Only diagonal (self-attentionë§Œ)
diagonal_ratio = np.diag(attn).sum() / attn.sum()
if diagonal_ratio > 0.8:
    print("âš ï¸  Model ignores past, only uses current state!")

# 3. Single timestep dominance
max_incoming = incoming.max()
if max_incoming > 3.0:  # >37.5% average
    print(f"âš ï¸  Over-reliance on timestep {incoming.argmax()}!")

# 4. Weak attention overall
if attn.max() < 0.15:
    print("âš ï¸  Weak attention weights - model may not be using attention effectively!")
```

---

## ğŸ”¬ ê³ ê¸‰ ë¶„ì„

### **Layerë³„ attention evolution:**

```python
# Load all layers
all_layers = data['attention_weights']  # (num_layers, num_heads, seq_len, seq_len)

fig, axes = plt.subplots(1, len(all_layers), figsize=(5*len(all_layers), 4))

for layer_idx, layer_attn in enumerate(all_layers):
    # Average over heads
    avg_attn = layer_attn.mean(axis=0)
    
    im = axes[layer_idx].imshow(avg_attn, cmap='hot', aspect='auto')
    axes[layer_idx].set_title(f'Layer {layer_idx}', fontsize=14, fontweight='bold')
    axes[layer_idx].set_xlabel('Key Position')
    axes[layer_idx].set_ylabel('Query Position')
    plt.colorbar(im, ax=axes[layer_idx])

plt.suptitle('Attention Evolution Across Layers', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('attention_layers.png', dpi=300)
```

### **Head specialization ë¶„ì„:**

```python
# Analyze each head in last layer
last_layer = data['attention_weights'][-1]  # (num_heads, seq_len, seq_len)

fig, axes = plt.subplots(2, 4, figsize=(16, 8))

for head_idx in range(8):
    row = head_idx // 4
    col = head_idx % 4
    
    head_attn = last_layer[head_idx]
    
    im = axes[row, col].imshow(head_attn, cmap='viridis', aspect='auto')
    axes[row, col].set_title(f'Head {head_idx}', fontweight='bold')
    plt.colorbar(im, ax=axes[row, col], fraction=0.046)

plt.suptitle('Head Specialization - Last Layer', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('attention_heads.png', dpi=300)
```

### **Batch ê°„ ë¹„êµ:**

```python
from pathlib import Path

# Load multiple batches
output_dir = Path('attention_output_dir')
npz_files = sorted(output_dir.glob('attention_batch_*.npz'))[:10]

all_temporal_imp = []
all_peak_timesteps = []

for npz_file in npz_files:
    data = np.load(npz_file)
    attn = data['attention_weights'][-1].mean(axis=0)
    
    temporal_imp = attn.sum(axis=0)
    all_temporal_imp.append(temporal_imp)
    
    peak_t = temporal_imp.argmax()
    all_peak_timesteps.append(peak_t)

# Average temporal importance
avg_temporal_imp = np.array(all_temporal_imp).mean(axis=0)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.bar(range(len(avg_temporal_imp)), avg_temporal_imp, 
        color='orangered', alpha=0.7)
plt.xlabel('Timestep', fontsize=12)
plt.ylabel('Average Temporal Importance', fontsize=12)
plt.title('Average Across Batches', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(all_peak_timesteps, bins=range(len(avg_temporal_imp)+1), 
         color='steelblue', alpha=0.7, edgecolor='black')
plt.xlabel('Peak Timestep', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Peak Timesteps', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('batch_comparison.png', dpi=300)

print(f"\nStatistics across {len(npz_files)} batches:")
print(f"  Most common peak: {max(set(all_peak_timesteps), key=all_peak_timesteps.count)}")
print(f"  Peak distribution: {np.bincount(all_peak_timesteps)}")
```

---

## ğŸ“š ì°¸ê³ : Attention Weight ê°’ í•´ì„í‘œ

| Weight | ì˜ë¯¸ | í•´ì„ | ì˜ˆì‹œ |
|--------|------|------|------|
| **0.30+** | ë§¤ìš° ê°•í•œ ì—°ê²° | ì§€ë°°ì  ê¸°ì—¬ (30%+) | Self-attention ê·¹ëŒ€ |
| **0.20-0.29** | ê°•í•œ ì—°ê²° | ì£¼ìš” ê¸°ì—¬ (20-29%) | Critical timestep |
| **0.125** | ê· ë“± ë¶„ì‚° | í‰ê·  ê¸°ì—¬ (1/8) | ê· ë“± ì°¸ì¡° |
| **0.08-0.12** | ì•½í•œ ì—°ê²° | ì†Œìˆ˜ ê¸°ì—¬ (8-12%) | ë°°ê²½ ì •ë³´ |
| **0.05-** | ë§¤ìš° ì•½í•œ ì—°ê²° | ë¬´ì‹œ ê°€ëŠ¥ (<5%) | ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨ |

---

## ğŸ“ Summary

### **í•µì‹¬ í¬ì¸íŠ¸:**

1. **Attention weightëŠ” í™•ë¥ **: Row sum = 1.0, ê° ê°’ = ê¸°ì—¬ë„ %
2. **Plot 1 (Matrix)**: ì „ì²´ attention íŒ¨í„´ (query Ã— key)
3. **Plot 2 (Bar)**: ê° timestepì´ ë°›ëŠ” ì´ attention (column sum)
4. **Plot 3 (Line)**: ê° queryì˜ attention ë¶„í¬ (rowë³„ ì‹œê°í™”)

### **ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸:**

- [ ] Row sumsê°€ ëª¨ë‘ 1.0ì¸ê°€? (í™•ë¥  ë¶„í¬ ê²€ì¦)
- [ ] Diagonalì´ ë°ì€ê°€? (Self-attention ê°•ë„)
- [ ] íŠ¹ì • timestepì´ ë°ì€ê°€? (Critical point ì¡´ì¬)
- [ ] Attentionì´ ë„ˆë¬´ uniformí•œê°€? (í•™ìŠµ ì‹¤íŒ¨ ì˜ì‹¬)
- [ ] Peak timestepì´ ë¬¼ë¦¬ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ê°€?

### **ë…¼ë¬¸ê³¼ ë¹„êµ:**

**DeepHalo (Zhang et al. 2025) ë°œê²¬:**
- **Positive prediction**: Uniform attention (progressive process)
- **Negative prediction**: Early-focused attention (early dismissal)

**Eunsuë‹˜ ëª¨ë¸:**
- Regression task â†’ ë” ë³µì¡í•œ íŒ¨í„´
- Multiple critical timesteps
- Self-attention + long-range dependency í˜¼í•©

---

## ğŸ“– ì¶”ê°€ ìë£Œ

### **ê´€ë ¨ ë…¼ë¬¸:**
- Zhang et al. (2025) - "Prediction of Halo CMEs Using Transformer Model"
- Vaswani et al. (2017) - "Attention Is All You Need"

### **ì½”ë“œ ì €ì¥ì†Œ:**
- `attention_analysis.py` - AttentionExtractor í´ë˜ìŠ¤
- `example_attention_all_targets.py` - ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `attention_0.yaml` - ì„¤ì • íŒŒì¼

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-01-22  
**ì‘ì„±ì:** Claude (Anthropic)  
**ë²„ì „:** 1.0
