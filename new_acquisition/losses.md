# Time Series Loss Functions Documentation

ì´ ë¬¸ì„œëŠ” ì´ìƒì¹˜ì™€ ê¸‰ê²©í•œ ë³€í™”ê°€ ì¤‘ìš”í•˜ê³ , ë¯¸ë˜ ì‹œì ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ì‹œê³„ì—´ íšŒê·€ ì‘ì—…ì„ ìœ„í•œ 6ê°€ì§€ ìš°ì„ ìˆœìœ„ë³„ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [Priority 1: Huber Multi-criteria Loss](#priority-1-huber-multi-criteria-loss)
2. [Priority 2: MAE Outlier-focused Loss](#priority-2-mae-outlier-focused-loss)
3. [Priority 3: Adaptive Weight Loss](#priority-3-adaptive-weight-loss)
4. [Priority 4: Gradient-based Weight Loss](#priority-4-gradient-based-weight-loss)
5. [Priority 5: Quantile Loss](#priority-5-quantile-loss)
6. [Priority 6: Multi-task Loss](#priority-6-multi-task-loss)
7. [ì‚¬ìš© ê°€ì´ë“œ](#ì‚¬ìš©-ê°€ì´ë“œ)
8. [í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹](#í•˜ì´í¼íŒŒë¼ë¯¸í„°-íŠœë‹)

---

## Priority 1: Huber Multi-criteria Loss

### ğŸ¥‡ ê°€ì¥ ì¶”ì²œë˜ëŠ” ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority1_HuberMultiCriteriaLoss`

### ê°œìš”
Huber Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì‹œê°„ì  ê°€ì¤‘ì¹˜(ë¯¸ë˜ ê°•ì¡°)ì™€ ë³€í™”ìœ¨ ê¸°ë°˜ ê°€ì¤‘ì¹˜(ê¸‰ë³€ ê°•ì¡°)ë¥¼ ê²°í•©í•œ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ìƒì¹˜ì— robustí•˜ë©´ì„œë„ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê· í˜•ì¡íŒ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**ê¸°ë³¸ Huber Loss:**
```
HuberLoss(x) = {
    0.5 * xÂ² / Î²,           if |x| â‰¤ Î²
    |x| - 0.5 * Î²,          if |x| > Î²
}
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = Î£áµ¢â±¼ [w_temporal(j) Ã— w_gradient(i,j) Ã— HuberLoss(pred(i,j) - target(i,j))]
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **Huber Loss ê¸°ë°˜**
- **ì¥ì **: MSEì™€ MAEì˜ ì¥ì ì„ ê²°í•©
- **ì†Œê·œëª¨ ì˜¤ì°¨**: L2 ì†ì‹¤ì²˜ëŸ¼ ë™ì‘ (ë¶€ë“œëŸ¬ìš´ ê·¸ë˜ë””ì–¸íŠ¸)
- **ëŒ€ê·œëª¨ ì˜¤ì°¨**: L1 ì†ì‹¤ì²˜ëŸ¼ ë™ì‘ (ì´ìƒì¹˜ì— robust)
- **Î² íŒŒë¼ë¯¸í„°**: ì „í™˜ì ì„ ì¡°ì • (ê¸°ë³¸ê°’: 0.3)

#### 2. **ì‹œê°„ì  ê°€ì¤‘ì¹˜ (Temporal Weighting)**
```python
temporal_weights = torch.linspace(start_weight, end_weight, seq_len)
```
- **ëª©ì **: ë¯¸ë˜ ì‹œì ì— ë” ë†’ì€ ì¤‘ìš”ë„ ë¶€ì—¬
- **êµ¬í˜„**: ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê°€ì¤‘ì¹˜
- **ê¸°ë³¸ê°’**: (0.3, 1.0) - ì‹œì‘ì  30%, ëì  100%

#### 3. **ë³€í™”ìœ¨ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (Gradient-based Weighting)**
```python
grad = torch.diff(target, dim=1)  # ì‹œê°„ì  ë³€í™”ìœ¨
grad_magnitude = torch.norm(grad, dim=2)  # ë³€í™”ì˜ í¬ê¸°
grad_weights = torch.softmax(grad_magnitude * scale, dim=1)
```
- **ëª©ì **: ê¸‰ê²©í•œ ë³€í™” êµ¬ê°„ì— ë” ë†’ì€ ì¤‘ìš”ë„ ë¶€ì—¬
- **êµ¬í˜„**: ì‹œê°„ì  ê·¸ë˜ë””ì–¸íŠ¸ì˜ í¬ê¸°ì— ë¹„ë¡€
- **ì •ê·œí™”**: Softmaxë¡œ ê°€ì¤‘ì¹˜ ë¶„í¬ ì¡°ì •

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `beta` | 0.3 | Huber loss ì „í™˜ì  | 0.1 - 1.0 |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |
| `gradient_weight_scale` | 2.0 | ë³€í™”ìœ¨ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ | 1.0 - 5.0 |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ì²« ë²ˆì§¸ ì„ íƒ**: ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ê¶Œì¥
- **ê· í˜•ì  ì ‘ê·¼**: ì•ˆì •ì„±ê³¼ ìš”êµ¬ì‚¬í•­ ë‹¬ì„±ì˜ ê· í˜•
- **ì‹¤ìš©ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… ì´ìƒì¹˜ì— robust
- âœ… ê¸‰ë³€ êµ¬ê°„ ê°•ì¡°
- âœ… ë¯¸ë˜ ì‹œì  ê°•ì¡°
- âœ… ì•ˆì •ì ì¸ í•™ìŠµ
- âœ… í•´ì„ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜

**ë‹¨ì :**
- âŒ ë§¤ìš° ê·¹ë‹¨ì ì¸ ì´ìƒì¹˜ì—ëŠ” ì—¬ì „íˆ ë¯¼ê°í•  ìˆ˜ ìˆìŒ
- âŒ ë‘ ê°œì˜ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ ì¡°ì • í•„ìš”

---

## Priority 2: MAE Outlier-focused Loss

### ğŸ¥ˆ ì´ìƒì¹˜ ì§‘ì¤‘ ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority2_MAEOutlierFocusedLoss`

### ê°œìš”
MAE Lossë¥¼ ê¸°ë°˜ìœ¼ë¡œ Z-scoreë¥¼ ì´ìš©í•œ ëª…ì‹œì  ì´ìƒì¹˜ íƒì§€ì™€ í•¨ê»˜ ë¯¸ë˜ ì‹œì  ê°€ì¤‘ì¹˜ë¥¼ ê²°í•©í•œ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ìƒì¹˜ êµ¬ê°„ì— ê°€ì¥ ì§ì ‘ì ìœ¼ë¡œ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**ê¸°ë³¸ MAE Loss:**
```
MAELoss(x) = |x|
```

**ì´ìƒì¹˜ íƒì§€:**
```
Z-score = |target - mean| / std
outlier_mask = Z-score > threshold
outlier_weight = 1 + mask Ã— (multiplier - 1)
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = Î£áµ¢â±¼ [w_temporal(j) Ã— w_outlier(i,j) Ã— MAELoss(pred(i,j) - target(i,j))]
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **MAE Loss ê¸°ë°˜**
- **ì™„ì „í•œ ì´ìƒì¹˜ ë‚´ì„±**: ì œê³±í•­ì´ ì—†ì–´ ê·¹ê°’ì— ëœ ë¯¼ê°
- **ì„ í˜• íŒ¨ë„í‹°**: ì˜¤ì°¨ì— ë¹„ë¡€í•˜ëŠ” ì¼ì •í•œ ê·¸ë˜ë””ì–¸íŠ¸
- **Robust íŠ¹ì„±**: ê°€ì¥ ì•ˆì •ì ì¸ ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜

#### 2. **Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€**
```python
mean = target.mean(dim=2, keepdim=True)
std = target.std(dim=2, keepdim=True) + 1e-8
z_scores = torch.abs((target - mean) / std)
max_z_score = z_scores.max(dim=2)[0]
outlier_mask = (max_z_score > threshold).float()
```
- **í†µê³„ì  ì ‘ê·¼**: í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì´ìƒì¹˜ ì •ì˜
- **ë™ì  íƒì§€**: ë°°ì¹˜ë³„ë¡œ ì´ìƒì¹˜ ê¸°ì¤€ ì¡°ì •
- **ì°¨ì›ë³„ ê³ ë ¤**: ëª¨ë“  feature ì°¨ì›ì„ ê³ ë ¤í•œ ì´ìƒì¹˜ íŒì •

#### 3. **ì´ìƒì¹˜ ê°€ì¤‘ì¹˜ ì¦í­**
```python
outlier_weights = 1.0 + outlier_mask * (multiplier - 1.0)
```
- **ì„ íƒì  ê°•ì¡°**: ì´ìƒì¹˜ë¡œ íŒì •ëœ ì‹œì ë§Œ ê°€ì¤‘ì¹˜ ì¦ê°€
- **ë°°ìˆ˜ ì¡°ì •**: multiplierë¡œ ê°•ì¡° ì •ë„ ì¡°ì ˆ

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `outlier_threshold` | 2.0 | Z-score ì´ìƒì¹˜ ì„ê³„ê°’ | 1.5 - 3.0 |
| `outlier_weight_multiplier` | 3.0 | ì´ìƒì¹˜ ê°€ì¤‘ì¹˜ ë°°ìˆ˜ | 2.0 - 5.0 |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ì´ìƒì¹˜ê°€ ë§¤ìš° ì¤‘ìš”í•œ ê²½ìš°**
- **ë…¸ì´ì¦ˆê°€ ì‹¬í•œ ë°ì´í„°**
- **Priority 1ìœ¼ë¡œ ì´ìƒì¹˜ ì„±ëŠ¥ì´ ë¶€ì¡±í•œ ê²½ìš°**
- **í•´ì„ ê°€ëŠ¥í•œ ì´ìƒì¹˜ íƒì§€ê°€ í•„ìš”í•œ ê²½ìš°**

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… ì´ìƒì¹˜ì— ê°€ì¥ robust
- âœ… ëª…ì‹œì  ì´ìƒì¹˜ íƒì§€
- âœ… í•´ì„ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜
- âœ… ì•ˆì •ì ì¸ í•™ìŠµ
- âœ… ê·¹í•œ ìƒí™©ì—ì„œë„ ì•ˆì •

**ë‹¨ì :**
- âŒ ì •ìƒ êµ¬ê°„ì—ì„œ ì„±ëŠ¥ì´ ë‹¤ì†Œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- âŒ Z-score ê¸°ë°˜ íƒì§€ì˜ í•œê³„
- âŒ ì´ìƒì¹˜ ê¸°ì¤€ì´ ê³ ì •ì 

---

## Priority 3: Adaptive Weight Loss

### ğŸ¥‰ ì ì‘í˜• ê°€ì¤‘ì¹˜ ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority3_AdaptiveWeightLoss`

### ê°œìš”
ì˜¤ì°¨ì˜ í¬ê¸°ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. í° ì˜¤ì°¨(ì´ìƒì¹˜/ê¸‰ë³€)ì¼ìˆ˜ë¡ ìë™ìœ¼ë¡œ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ í•™ìŠµ ê³¼ì •ì—ì„œ ì ì‘ì ìœ¼ë¡œ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**ì ì‘í˜• ê°€ì¤‘ì¹˜:**
```
error_magnitude = ||pred - target||â‚‚
adaptive_weight = (error_magnitude + Îµ)^power
normalized_weight = adaptive_weight / mean(adaptive_weight)
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = Î£áµ¢â±¼ [w_temporal(j) Ã— w_adaptive(i,j) Ã— BaseLoss(pred(i,j) - target(i,j))]
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •**
```python
error_magnitude = torch.norm(errors, dim=2)
adaptive_weights = torch.pow(error_magnitude + 1e-8, adaptive_power)
adaptive_weights = adaptive_weights / (adaptive_weights.mean(dim=1, keepdim=True) + 1e-8)
```
- **ìë™ íƒì§€**: ë³„ë„ì˜ ì„ê³„ê°’ ì„¤ì • ì—†ì´ ì˜¤ì°¨ í¬ê¸°ë¡œ ìë™ íŒë‹¨
- **ì—°ì†ì  ê°€ì¤‘ì¹˜**: ì´ì§„ ë§ˆìŠ¤í¬ê°€ ì•„ë‹Œ ì—°ì†ì ì¸ ê°€ì¤‘ì¹˜
- **ì •ê·œí™”**: ì‹œí€€ìŠ¤ë³„ ì •ê·œí™”ë¡œ ì•ˆì •ì„± í™•ë³´

#### 2. **Power Law ìŠ¤ì¼€ì¼ë§**
- **Adaptive Power**: ê°€ì¤‘ì¹˜ ì¦ê°€ ì†ë„ ì¡°ì ˆ
- **Power < 1**: ë¶€ë“œëŸ¬ìš´ ê°€ì¤‘ì¹˜ ì¦ê°€
- **Power > 1**: ê¸‰ê²©í•œ ê°€ì¤‘ì¹˜ ì¦ê°€
- **Power = 1**: ì„ í˜• ì¦ê°€

#### 3. **ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜ ì„ íƒ**
```python
if base_loss_type == 'mse':
    self.base_loss_fn = nn.MSELoss(reduction='none')
elif base_loss_type == 'mae':
    self.base_loss_fn = nn.L1Loss(reduction='none')
elif base_loss_type == 'huber':
    self.base_loss_fn = nn.SmoothL1Loss(beta=beta, reduction='none')
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `base_loss_type` | 'huber' | ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜ íƒ€ì… | 'mse', 'mae', 'huber' |
| `beta` | 0.5 | Huber loss íŒŒë¼ë¯¸í„° | 0.1 - 1.0 |
| `adaptive_power` | 1.5 | ì ì‘í˜• ê°€ì¤‘ì¹˜ ì§€ìˆ˜ | 1.0 - 2.5 |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ìë™í™”ëœ ê°€ì¤‘ì¹˜ ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš°**
- **ì´ìƒì¹˜ì˜ ì •ì˜ê°€ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°**
- **ë³µì¡í•œ íŒ¨í„´ì˜ ë°ì´í„°**
- **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œê°„ì´ ì œí•œì ì¸ ê²½ìš°**

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… ìë™ ê°€ì¤‘ì¹˜ ì¡°ì •
- âœ… ì—°ì†ì ì¸ ì¤‘ìš”ë„ ë¶€ì—¬
- âœ… ë‹¤ì–‘í•œ ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜ ì§€ì›
- âœ… ë°ì´í„° ì ì‘ì 

**ë‹¨ì :**
- âŒ êµ¬í˜„ ë³µì¡ë„ ë†’ìŒ
- âŒ í•´ì„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì–´ë ¤ì›€
- âŒ ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë²„í—¤ë“œ

---

## Priority 4: Gradient-based Weight Loss

### ë³€í™”ìœ¨ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority4_GradientBasedWeightLoss`

### ê°œìš”
ì‹œê°„ì  ë³€í™”ìœ¨(gradient)ì˜ í¬ê¸°ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ê¸‰ê²©í•œ ë³€í™”ê°€ ì¼ì–´ë‚˜ëŠ” ì‹œì ì— ì§‘ì¤‘ì ìœ¼ë¡œ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**ì‹œê°„ì  ê·¸ë˜ë””ì–¸íŠ¸:**
```
grad(t) = target(t+1) - target(t)
grad_magnitude = ||grad(t)||â‚‚
```

**ê·¸ë˜ë””ì–¸íŠ¸ ê°€ì¤‘ì¹˜:**
```
gradient_weight = exp(grad_magnitude Ã— scale / max(grad_magnitude))
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = Î£áµ¢â±¼ [w_temporal(j) Ã— w_gradient(i,j) Ã— BaseLoss(pred(i,j) - target(i,j))]
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **ì‹œê°„ì  ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°**
```python
grad = torch.diff(target, dim=1)  # ì—°ì† ì‹œì  ê°„ ì°¨ì´
grad_magnitude = torch.norm(grad, dim=2)  # ë³€í™”ëŸ‰ í¬ê¸°
grad_magnitude = F.pad(grad_magnitude, (1, 0), value=0)  # íŒ¨ë”©ìœ¼ë¡œ ê¸¸ì´ ë§ì¶¤
```
- **ì°¨ë¶„ ê³„ì‚°**: ì—°ì†ëœ ì‹œì  ê°„ì˜ ë³€í™”ëŸ‰ ê³„ì‚°
- **ë…¸ë¦„ ê³„ì‚°**: ë‹¤ì°¨ì› ë³€í™”ëŸ‰ì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜
- **íŒ¨ë”© ì²˜ë¦¬**: ì›ë˜ ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë§ì¶¤

#### 2. **ì§€ìˆ˜ì  ê°€ì¤‘ì¹˜**
```python
gradient_weights = torch.exp(grad_magnitude Ã— scale / (max_grad + Îµ))
```
- **ì§€ìˆ˜ í•¨ìˆ˜**: ê¸‰ë³€ êµ¬ê°„ì„ ê°•í•˜ê²Œ ê°•ì¡°
- **ì •ê·œí™”**: ìµœëŒ€ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì•ˆì •ì„± í™•ë³´
- **í´ë¨í•‘**: ê·¹ë‹¨ì ì¸ ê°€ì¤‘ì¹˜ ë°©ì§€

#### 3. **ì§ì ‘ì  ê¸‰ë³€ íƒ€ê²ŸíŒ…**
- **ëª…í™•í•œ ëª©ì **: ë³€í™”ìœ¨ì—ë§Œ ì§‘ì¤‘
- **ì¦‰ê°ì  ë°˜ì‘**: ë³€í™”ê°€ ê°ì§€ë˜ëŠ” ì¦‰ì‹œ ê°€ì¤‘ì¹˜ ì¦ê°€
- **ì‹œê°„ì  ì—°ì†ì„±**: ì—°ì†ëœ ì‹œì ì˜ ê´€ê³„ ê³ ë ¤

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `base_loss_type` | 'mae' | ê¸°ë³¸ ì†ì‹¤í•¨ìˆ˜ íƒ€ì… | 'mse', 'mae', 'huber' |
| `beta` | 0.5 | Huber loss íŒŒë¼ë¯¸í„° | 0.1 - 1.0 |
| `gradient_weight_scale` | 3.0 | ê·¸ë˜ë””ì–¸íŠ¸ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ | 1.0 - 5.0 |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ê¸‰ê²©í•œ ë³€í™”ê°€ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•ì¸ ê²½ìš°**
- **ì‹œê³„ì—´ì˜ ë³€í™”ì  íƒì§€ê°€ ëª©ì ì¸ ê²½ìš°**
- **ì´ìƒì¹˜ë³´ë‹¤ ë³€í™”ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°**
- **ì‹œê°„ì  íŒ¨í„´ì´ ì¤‘ìš”í•œ ê²½ìš°**

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… ê¸‰ë³€ êµ¬ê°„ ì§ì ‘ íƒ€ê²ŸíŒ…
- âœ… ì‹œê°„ì  ì—°ì†ì„± ê³ ë ¤
- âœ… ëª…í™•í•œ ëª©ì ì„±
- âœ… ë³€í™”ì  íƒì§€ì— íš¨ê³¼ì 

**ë‹¨ì :**
- âŒ ë³€í™”ìœ¨ì´ ì‘ì€ ì´ìƒì¹˜ ë†“ì¹  ìˆ˜ ìˆìŒ
- âŒ ë…¸ì´ì¦ˆì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ
- âŒ ì²« ë²ˆì§¸ ì‹œì  ê°€ì¤‘ì¹˜ í•­ìƒ 0

---

## Priority 5: Quantile Loss

### ë¶„ìœ„ìˆ˜ íšŒê·€ ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority5_QuantileLoss`

### ê°œìš”
ì˜ˆì¸¡ êµ¬ê°„ì„ ì œê³µí•˜ëŠ” ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ ìœ„í•œ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ êµ¬ê°„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ë©°, ì  ì¶”ì •ê°’ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ˆì¸¡ êµ¬ê°„ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**ë¶„ìœ„ìˆ˜ ì†ì‹¤:**
```
QuantileLoss(Ï„) = max(Ï„ Ã— (y - Å·), (Ï„ - 1) Ã— (y - Å·))
```
ì—¬ê¸°ì„œ Ï„ëŠ” ë¶„ìœ„ìˆ˜ (0 < Ï„ < 1)

**ë¶ˆí™•ì‹¤ì„± ê°€ì¤‘ì¹˜:**
```
uncertainty = ||q_high - q_low||â‚‚  # ë¶„ìœ„ìˆ˜ ê°„ ê±°ë¦¬
uncertainty_weight = 1 + uncertainty Ã— scale
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = Î£áµ¢â±¼Ï„ [w_temporal(j) Ã— w_uncertainty(i,j) Ã— QuantileLoss_Ï„(pred_Ï„(i,j), target(i,j))]
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **ë‹¤ì¤‘ ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡**
```python
for i, quantile in enumerate(self.quantiles):
    pred_q = pred[..., i]
    q_loss = self._quantile_loss(pred_q, target, quantile)
    total_loss += q_loss
```
- **ë™ì‹œ í•™ìŠµ**: ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ í•œ ë²ˆì— í•™ìŠµ
- **ì˜ˆì¸¡ êµ¬ê°„**: ì‹ ë¢°êµ¬ê°„ ì œê³µ ê°€ëŠ¥
- **ê¸°ë³¸ ë¶„ìœ„ìˆ˜**: [0.1, 0.5, 0.9] (10%, 50%, 90%)

#### 2. **ë¹„ëŒ€ì¹­ ì†ì‹¤í•¨ìˆ˜**
```python
def _quantile_loss(self, pred, target, quantile):
    errors = target - pred
    return torch.maximum(quantile * errors, (quantile - 1) * errors)
```
- **ë¶„ìœ„ìˆ˜ë³„ íŠ¹ì„±**: ê° ë¶„ìœ„ìˆ˜ì— ë§ëŠ” ë¹„ëŒ€ì¹­ íŒ¨ë„í‹°
- **ìƒí•œ/í•˜í•œ**: ê³¼ì†Œ/ê³¼ëŒ€ ì˜ˆì¸¡ì— ë‹¤ë¥¸ íŒ¨ë„í‹°
- **í™•ë¥ ì  í•´ì„**: ë¶„ìœ„ìˆ˜ì˜ í™•ë¥ ì  ì˜ë¯¸ ë°˜ì˜

#### 3. **ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ê°€ì¤‘ì¹˜**
```python
uncertainty = torch.norm(q_high - q_low, dim=2)  # IQR
uncertainty_weights = 1.0 + uncertainty * scale
```
- **êµ¬ê°„ ë„ˆë¹„**: ë¶„ìœ„ìˆ˜ ê°„ ê±°ë¦¬ë¡œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •
- **ì ì‘ì  ê°€ì¤‘ì¹˜**: ë¶ˆí™•ì‹¤í•œ êµ¬ê°„ì— ë” ë†’ì€ ì¤‘ìš”ë„
- **í•´ì„ ê°€ëŠ¥ì„±**: ë„“ì€ ì˜ˆì¸¡ êµ¬ê°„ = ë†’ì€ ë¶ˆí™•ì‹¤ì„±

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `quantiles` | [0.1, 0.5, 0.9] | ì˜ˆì¸¡í•  ë¶„ìœ„ìˆ˜ ëª©ë¡ | ë‹¤ì–‘í•œ ì¡°í•© ê°€ëŠ¥ |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |
| `uncertainty_weight_scale` | 2.0 | ë¶ˆí™•ì‹¤ì„± ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ | 1.0 - 5.0 |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°**
- **ì‹ ë¢°êµ¬ê°„ì´ í•„ìš”í•œ ê²½ìš°**
- **ë¦¬ìŠ¤í¬ ê´€ë¦¬ê°€ ì¤‘ìš”í•œ ì‘ìš© ë¶„ì•¼**
- **í™•ë¥ ì  ì˜ˆì¸¡ì´ í•„ìš”í•œ ê²½ìš°**

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… ì˜ˆì¸¡ êµ¬ê°„ ì œê³µ
- âœ… ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
- âœ… í™•ë¥ ì  í•´ì„ ê°€ëŠ¥
- âœ… ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìš©ì´

**ë‹¨ì :**
- âŒ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ ë³€ê²½ í•„ìš”
- âŒ ë‹¨ìˆœ íšŒê·€ë³´ë‹¤ ë³µì¡
- âŒ ë¶„ìœ„ìˆ˜ ì„ íƒì˜ ì–´ë ¤ì›€
- âŒ ê³„ì‚° ì˜¤ë²„í—¤ë“œ ì¦ê°€

---

## Priority 6: Multi-task Loss

### ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ ì†ì‹¤í•¨ìˆ˜

**í´ë˜ìŠ¤ëª…**: `Priority6_MultiTaskLoss`

### ê°œìš”
íšŒê·€ ì‘ì—…ê³¼ ì´ìƒì¹˜ íƒì§€ ì‘ì—…ì„ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. ë³´ì¡° ì‘ì—…(ì´ìƒì¹˜ íƒì§€)ì„ í†µí•´ ì´ìƒì¹˜ êµ¬ê°„ì—ì„œì˜ íšŒê·€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### ìˆ˜í•™ì  ì •ì˜

**íšŒê·€ ì†ì‹¤:**
```
L_regression = BaseLoss(pred_regression, target)
```

**ì´ìƒì¹˜ íƒì§€ ì†ì‹¤:**
```
L_outlier = BCEWithLogitsLoss(pred_outlier, outlier_labels)
```

**ì „ì²´ ì†ì‹¤í•¨ìˆ˜:**
```
L = w_temporal Ã— (L_regression + Î» Ã— L_outlier)
```

### ì£¼ìš” íŠ¹ì§•

#### 1. **ë™ì‹œ ë‹¤ì¤‘ ì‘ì—…**
```python
# íšŒê·€ ì†ì‹¤
regression_loss = self.regression_loss_fn(pred, target)

# ì´ìƒì¹˜ íƒì§€ ì†ì‹¤
if outlier_logits is not None:
    outlier_labels = self._generate_outlier_labels(target)
    outlier_loss = self.outlier_loss_fn(outlier_logits, outlier_labels)
```
- **ì£¼ ì‘ì—…**: ì‹œê³„ì—´ íšŒê·€
- **ë³´ì¡° ì‘ì—…**: ì´ìƒì¹˜ ì´ì§„ ë¶„ë¥˜
- **ìƒí˜¸ ë³´ì™„**: ë‘ ì‘ì—…ì´ ì„œë¡œ ë„ì›€

#### 2. **ìë™ ì´ìƒì¹˜ ë¼ë²¨ ìƒì„±**
```python
def _generate_outlier_labels(self, target):
    mean = target.mean(dim=2, keepdim=True)
    std = target.std(dim=2, keepdim=True) + 1e-8
    z_scores = torch.abs((target - mean) / std)
    max_z_score = z_scores.max(dim=2)[0]
    outlier_labels = (max_z_score > self.outlier_threshold).float()
    return outlier_labels
```
- **Z-score ê¸°ë°˜**: í†µê³„ì  ì´ìƒì¹˜ ì •ì˜
- **ë™ì  ìƒì„±**: í•™ìŠµ ì¤‘ ìë™ìœ¼ë¡œ ë¼ë²¨ ìƒì„±
- **ì´ì§„ ë¶„ë¥˜**: ì´ìƒì¹˜/ì •ìƒ êµ¬ë¶„

#### 3. **ê°€ì¤‘ì¹˜ ê· í˜•**
```python
total_loss = weighted_regression_loss + self.outlier_loss_weight * weighted_outlier_loss
```
- **ê°€ì¤‘ì¹˜ ì¡°ì •**: ë‘ ì‘ì—… ê°„ ê· í˜• ì¡°ì ˆ
- **ì£¼/ë³´ì¡° ê´€ê³„**: íšŒê·€ê°€ ì£¼, ì´ìƒì¹˜ íƒì§€ê°€ ë³´ì¡°
- **ì•ˆì •í™”**: ë³´ì¡° ì‘ì—…ì´ ì£¼ ì‘ì—… ì•ˆì •í™”

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | íŠœë‹ ë²”ìœ„ |
|---------|--------|------|-----------|
| `regression_loss_type` | 'huber' | íšŒê·€ ì†ì‹¤í•¨ìˆ˜ íƒ€ì… | 'mse', 'mae', 'huber' |
| `beta` | 0.5 | Huber loss íŒŒë¼ë¯¸í„° | 0.1 - 1.0 |
| `outlier_loss_weight` | 0.3 | ì´ìƒì¹˜ íƒì§€ ì†ì‹¤ ê°€ì¤‘ì¹˜ | 0.1 - 0.5 |
| `temporal_weight_range` | (0.3, 1.0) | ì‹œê°„ ê°€ì¤‘ì¹˜ ë²”ìœ„ | (0.1-0.5, 0.8-1.0) |
| `outlier_threshold` | 2.0 | ì´ìƒì¹˜ Z-score ì„ê³„ê°’ | 1.5 - 3.0 |

### ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?
- **ì´ìƒì¹˜ íƒì§€ ëŠ¥ë ¥ì´ í•„ìš”í•œ ê²½ìš°**
- **ë³µì¡í•œ ëª¨ë¸ë¡œ ì‹¤í—˜í•  ì—¬ìœ ê°€ ìˆëŠ” ê²½ìš°**
- **ì´ìƒì¹˜ êµ¬ê°„ ì‹ë³„ì´ ë¶€ì°¨ì  ëª©í‘œì¸ ê²½ìš°**
- **ë‹¤ì–‘í•œ ì¶œë ¥ì´ í•„ìš”í•œ ê²½ìš°**

### ì¥ë‹¨ì 

**ì¥ì :**
- âœ… í¬ê´„ì ì¸ ì ‘ê·¼
- âœ… ì´ìƒì¹˜ íƒì§€ ëŠ¥ë ¥ í–¥ìƒ
- âœ… ìƒí˜¸ ë³´ì™„ì  í•™ìŠµ
- âœ… ë‹¤ì–‘í•œ ì¶œë ¥ ì œê³µ

**ë‹¨ì :**
- âŒ ëª¨ë¸ ë³µì¡ë„ ì¦ê°€
- âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë³µì¡
- âŒ í•™ìŠµ ì‹œê°„ ì¦ê°€
- âŒ êµ¬í˜„ ë³µì¡ë„ ë†’ìŒ

---

## ì‚¬ìš© ê°€ì´ë“œ

### ğŸ“‹ ì„ íƒ ê°€ì´ë“œ

#### 1. **ë¹ ë¥¸ ì‹œì‘ (ê¶Œì¥)**
```python
# ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ì í•©
loss_fn = Priority1_HuberMultiCriteriaLoss(
    beta=0.3,
    temporal_weight_range=(0.3, 1.0),
    gradient_weight_scale=2.0
)
```

#### 2. **ì´ìƒì¹˜ê°€ ë§¤ìš° ì¤‘ìš”í•œ ê²½ìš°**
```python
# ì´ìƒì¹˜ ì§‘ì¤‘
loss_fn = Priority2_MAEOutlierFocusedLoss(
    outlier_threshold=2.0,
    outlier_weight_multiplier=3.0
)
```

#### 3. **ìë™í™”ë¥¼ ì›í•˜ëŠ” ê²½ìš°**
```python
# ì ì‘í˜• ê°€ì¤‘ì¹˜
loss_fn = Priority3_AdaptiveWeightLoss(
    adaptive_power=1.5,
    base_loss_type='huber'
)
```

### ğŸ”„ ì‹¤í—˜ ìˆœì„œ

1. **Priority 1**ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ ë² ì´ìŠ¤ë¼ì¸ ì„¤ì •
2. **ì´ìƒì¹˜ ì„±ëŠ¥**ì´ ë¶€ì¡±í•˜ë©´ **Priority 2** ì‹œë„
3. **ë³µì¡í•œ íŒ¨í„´**ì´ë©´ **Priority 3** ì‹œë„
4. **ê¸‰ë³€ì´ í•µì‹¬**ì´ë©´ **Priority 4** ì‹œë„
5. **ë¶ˆí™•ì‹¤ì„±**ì´ ì¤‘ìš”í•˜ë©´ **Priority 5** ì‹œë„
6. **í¬ê´„ì  ì ‘ê·¼**ì´ í•„ìš”í•˜ë©´ **Priority 6** ì‹œë„

### ğŸ“Š ì„±ëŠ¥ í‰ê°€

#### ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
- **ì „ì²´ MSE/MAE**: ì¼ë°˜ì ì¸ íšŒê·€ ì„±ëŠ¥
- **ê°€ì¤‘ ì†ì‹¤ê°’**: ì‚¬ìš©ì ì •ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’

#### êµ¬ê°„ë³„ ì„±ëŠ¥ ì§€í‘œ
- **ì´ìƒì¹˜ êµ¬ê°„ ì„±ëŠ¥**: ì´ìƒì¹˜ë¡œ ë¶„ë¥˜ëœ ì‹œì ë“¤ì˜ ì„±ëŠ¥
- **ê¸‰ë³€ êµ¬ê°„ ì„±ëŠ¥**: ë†’ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì‹œì ë“¤ì˜ ì„±ëŠ¥
- **ë¯¸ë˜ ì‹œì  ì„±ëŠ¥**: ì‹œí€€ìŠ¤ í›„ë°˜ë¶€ì˜ ì„±ëŠ¥
- **ì •ìƒ êµ¬ê°„ ì„±ëŠ¥**: ì¼ë°˜ì ì¸ ì‹œì ë“¤ì˜ ì„±ëŠ¥

#### ì„±ëŠ¥ í‰ê°€ ì½”ë“œ ì˜ˆì‹œ
```python
def evaluate_performance(loss_fn, pred, target):
    """ì†ì‹¤í•¨ìˆ˜ë³„ ì„¸ë¶€ ì„±ëŠ¥ í‰ê°€"""
    
    # ì „ì²´ ì„±ëŠ¥
    total_loss = loss_fn(pred, target)
    mse = F.mse_loss(pred, target)
    mae = F.l1_loss(pred, target)
    
    # ì´ìƒì¹˜ êµ¬ê°„ ì‹ë³„
    mean = target.mean(dim=2, keepdim=True)
    std = target.std(dim=2, keepdim=True) + 1e-8
    z_scores = torch.abs((target - mean) / std)
    outlier_mask = z_scores.max(dim=2)[0] > 2.0
    
    # ê¸‰ë³€ êµ¬ê°„ ì‹ë³„
    grad = torch.diff(target, dim=1)
    grad_magnitude = torch.norm(grad, dim=2)
    grad_magnitude = F.pad(grad_magnitude, (1, 0), value=0)
    rapid_change_mask = grad_magnitude > grad_magnitude.quantile(0.8, dim=1, keepdim=True)
    
    # ë¯¸ë˜ ì‹œì  (í›„ë°˜ 30%)
    seq_len = target.shape[1]
    future_mask = torch.zeros_like(outlier_mask)
    future_mask[:, int(seq_len*0.7):] = 1
    
    # êµ¬ê°„ë³„ ì„±ëŠ¥ ê³„ì‚°
    results = {
        'total_loss': total_loss.item(),
        'total_mse': mse.item(),
        'total_mae': mae.item(),
        'outlier_mse': F.mse_loss(pred[outlier_mask], target[outlier_mask]).item() if outlier_mask.any() else 0,
        'rapid_change_mse': F.mse_loss(pred[rapid_change_mask], target[rapid_change_mask]).item() if rapid_change_mask.any() else 0,
        'future_mse': F.mse_loss(pred[future_mask.bool()], target[future_mask.bool()]).item(),
        'outlier_ratio': outlier_mask.float().mean().item(),
        'rapid_change_ratio': rapid_change_mask.float().mean().item()
    }
    
    return results
```

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ğŸ¯ íŠœë‹ ì „ëµ

#### 1ë‹¨ê³„: ê¸°ë³¸ ì„¤ì •
```python
# ëª¨ë“  ì†ì‹¤í•¨ìˆ˜ì˜ ê³µí†µ ì‹œì‘ì 
common_config = {
    'temporal_weight_range': (0.3, 1.0),  # ë¯¸ë˜ ê°•ì¡° ê¸°ë³¸ê°’
    'reduction': 'mean'
}
```

#### 2ë‹¨ê³„: ì†ì‹¤í•¨ìˆ˜ë³„ í•µì‹¬ íŒŒë¼ë¯¸í„° íŠœë‹

**Priority 1 - Huber Multi-criteria**
```python
# ê·¸ë¦¬ë“œ ì„œì¹˜ ë²”ìœ„
param_grid = {
    'beta': [0.1, 0.3, 0.5, 0.7, 1.0],
    'gradient_weight_scale': [1.0, 2.0, 3.0, 4.0, 5.0],
    'temporal_weight_range': [(0.2, 1.0), (0.3, 1.0), (0.4, 1.0)]
}

# íŠœë‹ ìˆœì„œ
# 1. beta ë¨¼ì € ì¡°ì • (ì´ìƒì¹˜ ë¯¼ê°ë„)
# 2. gradient_weight_scale ì¡°ì • (ê¸‰ë³€ ê°•ì¡°)
# 3. temporal_weight_range ë¯¸ì„¸ ì¡°ì •
```

**Priority 2 - MAE Outlier-focused**
```python
param_grid = {
    'outlier_threshold': [1.5, 2.0, 2.5, 3.0],
    'outlier_weight_multiplier': [2.0, 3.0, 4.0, 5.0],
    'temporal_weight_range': [(0.2, 1.0), (0.3, 1.0), (0.4, 1.0)]
}

# íŠœë‹ ìˆœì„œ
# 1. outlier_threshold ì¡°ì • (ì´ìƒì¹˜ íƒì§€ ë¯¼ê°ë„)
# 2. outlier_weight_multiplier ì¡°ì • (ì´ìƒì¹˜ ê°•ì¡° ì •ë„)
# 3. temporal_weight_range ë¯¸ì„¸ ì¡°ì •
```

**Priority 3 - Adaptive Weight**
```python
param_grid = {
    'base_loss_type': ['mse', 'mae', 'huber'],
    'adaptive_power': [1.0, 1.2, 1.5, 1.8, 2.0],
    'beta': [0.3, 0.5, 0.7],  # huberì¸ ê²½ìš°ë§Œ
}

# íŠœë‹ ìˆœì„œ
# 1. base_loss_type ì„ íƒ
# 2. adaptive_power ì¡°ì • (ì ì‘í˜• ê°•ë„)
# 3. beta ì¡°ì • (huberì¸ ê²½ìš°)
```

#### 3ë‹¨ê³„: ì„¸ë¶€ íŠœë‹

**ì‹œê°„ ê°€ì¤‘ì¹˜ ì„¸ë¶€ ì¡°ì •**
```python
# ë¯¸ë˜ ê°•ì¡° ì •ë„ì— ë”°ë¥¸ ì˜µì…˜
temporal_options = {
    'weak_future_emphasis': (0.4, 1.0),     # ì•½í•œ ë¯¸ë˜ ê°•ì¡°
    'medium_future_emphasis': (0.3, 1.0),   # ì¤‘ê°„ ë¯¸ë˜ ê°•ì¡° (ê¸°ë³¸)
    'strong_future_emphasis': (0.2, 1.0),   # ê°•í•œ ë¯¸ë˜ ê°•ì¡°
    'very_strong_emphasis': (0.1, 1.0),     # ë§¤ìš° ê°•í•œ ë¯¸ë˜ ê°•ì¡°
}

# ì§€ìˆ˜ì  ê°€ì¤‘ì¹˜ ì˜µì…˜ (ê³ ê¸‰)
def exponential_weights(seq_len, decay_rate=0.95):
    """ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì‹œê°„ ê°€ì¤‘ì¹˜"""
    weights = torch.exp(torch.linspace(0, -np.log(decay_rate), seq_len))
    return weights / weights.min()  # ìµœì†Œê°’ì„ 1ë¡œ ì •ê·œí™”
```

### ğŸ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§
```python
class LossMonitor:
    """ì†ì‹¤í•¨ìˆ˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.history = {
            'total_loss': [],
            'outlier_loss': [],
            'rapid_change_loss': [],
            'future_loss': [],
            'normal_loss': []
        }
    
    def update(self, loss_fn, pred, target, step):
        """ê° ë‹¨ê³„ë³„ ì„¸ë¶€ ì†ì‹¤ ê¸°ë¡"""
        
        # ì „ì²´ ì†ì‹¤
        total_loss = loss_fn(pred, target)
        self.history['total_loss'].append((step, total_loss.item()))
        
        # êµ¬ê°„ë³„ ì†ì‹¤ ê³„ì‚° ë° ê¸°ë¡
        outlier_mask = self._detect_outliers(target)
        rapid_mask = self._detect_rapid_changes(target)
        future_mask = self._get_future_mask(target)
        normal_mask = ~(outlier_mask | rapid_mask)
        
        # ê° êµ¬ê°„ë³„ ì†ì‹¤ ê¸°ë¡
        if outlier_mask.any():
            outlier_loss = F.mse_loss(pred[outlier_mask], target[outlier_mask])
            self.history['outlier_loss'].append((step, outlier_loss.item()))
        
        # ... ë‹¤ë¥¸ êµ¬ê°„ë“¤ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    
    def plot_progress(self):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ì‹œê°í™”"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        
        for i, (loss_type, values) in enumerate(self.history.items()):
            if values:
                steps, losses = zip(*values)
                ax = axes[i//3, i%3]
                ax.plot(steps, losses, label=loss_type)
                ax.set_title(f'{loss_type} Progress')
                ax.set_xlabel('Step')
                ax.set_ylabel('Loss')
                ax.legend()
        
        plt.tight_layout()
        plt.show()
```

#### ê²€ì¦ ì „ëµ
```python
def cross_validate_loss_functions(X, y, loss_functions, cv_folds=5):
    """ì—¬ëŸ¬ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ êµì°¨ ê²€ì¦"""
    
    results = {}
    
    for name, loss_fn in loss_functions.items():
        fold_results = []
        
        for fold in range(cv_folds):
            # ë°ì´í„° ë¶„í• 
            train_X, val_X, train_y, val_y = train_test_split(X, y, fold)
            
            # ëª¨ë¸ í•™ìŠµ
            model = YourModel()
            optimizer = torch.optim.Adam(model.parameters())
            
            for epoch in range(num_epochs):
                pred = model(train_X)
                loss = loss_fn(pred, train_y)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            
            # ê²€ì¦ ì„±ëŠ¥
            with torch.no_grad():
                val_pred = model(val_X)
                val_metrics = evaluate_performance(loss_fn, val_pred, val_y)
                fold_results.append(val_metrics)
        
        # í´ë“œë³„ ê²°ê³¼ ì§‘ê³„
        results[name] = {
            metric: np.mean([fold[metric] for fold in fold_results])
            for metric in fold_results[0].keys()
        }
    
    return results
```

### ğŸ“ˆ ê³ ê¸‰ íŠœë‹ ê¸°ë²•

#### 1. **ë² ì´ì§€ì•ˆ ìµœì í™”**
```python
from skopt import gp_minimize
from skopt.space import Real, Categorical

def objective(params):
    """ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ëª©ì  í•¨ìˆ˜"""
    
    # íŒŒë¼ë¯¸í„° ì–¸íŒ©
    beta, gradient_scale, temp_start = params
    
    # ì†ì‹¤í•¨ìˆ˜ ì„¤ì •
    loss_fn = Priority1_HuberMultiCriteriaLoss(
        beta=beta,
        gradient_weight_scale=gradient_scale,
        temporal_weight_range=(temp_start, 1.0)
    )
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    val_loss = train_and_evaluate(loss_fn)
    
    return val_loss

# íƒìƒ‰ ê³µê°„ ì •ì˜
space = [
    Real(0.1, 1.0, name='beta'),
    Real(1.0, 5.0, name='gradient_scale'),
    Real(0.1, 0.5, name='temp_start')
]

# ìµœì í™” ì‹¤í–‰
result = gp_minimize(objective, space, n_calls=50)
best_params = result.x
```

#### 2. **ë™ì  ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¤„ë§**
```python
class DynamicWeightScheduler:
    """í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •"""
    
    def __init__(self, loss_fn, schedule_type='cosine'):
        self.loss_fn = loss_fn
        self.schedule_type = schedule_type
        self.initial_params = self._get_current_params()
    
    def step(self, epoch, total_epochs):
        """ì—í¬í¬ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì¡°ì •"""
        
        if self.schedule_type == 'cosine':
            # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ë§
            factor = 0.5 * (1 + np.cos(np.pi * epoch / total_epochs))
        elif self.schedule_type == 'linear':
            # ì„ í˜• ê°ì†Œ
            factor = 1 - epoch / total_epochs
        
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        if hasattr(self.loss_fn, 'gradient_weight_scale'):
            self.loss_fn.gradient_weight_scale = self.initial_params['gradient_scale'] * (1 + factor)
        
        if hasattr(self.loss_fn, 'outlier_weight_multiplier'):
            self.loss_fn.outlier_weight_multiplier = self.initial_params['outlier_multiplier'] * (1 + factor)

# ì‚¬ìš© ì˜ˆì‹œ
scheduler = DynamicWeightScheduler(loss_fn, 'cosine')

for epoch in range(num_epochs):
    scheduler.step(epoch, num_epochs)
    
    # ì¼ë°˜ì ì¸ í•™ìŠµ ë£¨í”„
    for batch in dataloader:
        pred = model(batch.x)
        loss = loss_fn(pred, batch.y)
        # ... ì—­ì „íŒŒ ë“±
```

#### 3. **ì•™ìƒë¸” ì†ì‹¤í•¨ìˆ˜**
```python
class EnsembleLoss(nn.Module):
    """ì—¬ëŸ¬ ì†ì‹¤í•¨ìˆ˜ì˜ ê°€ì¤‘ í‰ê· """
    
    def __init__(self, loss_functions, weights=None):
        super().__init__()
        self.loss_functions = loss_functions
        self.weights = weights or [1.0] * len(loss_functions)
        
    def forward(self, pred, target):
        total_loss = 0
        
        for loss_fn, weight in zip(self.loss_functions, self.weights):
            loss = loss_fn(pred, target)
            total_loss += weight * loss
        
        return total_loss / sum(self.weights)

# ì‚¬ìš© ì˜ˆì‹œ
ensemble_loss = EnsembleLoss([
    Priority1_HuberMultiCriteriaLoss(beta=0.3),
    Priority2_MAEOutlierFocusedLoss(outlier_threshold=2.0)
], weights=[0.7, 0.3])
```

### ğŸš¨ ì£¼ì˜ì‚¬í•­ ë° íŒ

#### ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ë“¤

1. **ê³¼ë„í•œ ê°€ì¤‘ì¹˜ ì„¤ì •**
   ```python
   # ì˜ëª»ëœ ì˜ˆì‹œ
   loss_fn = Priority1_HuberMultiCriteriaLoss(
       gradient_weight_scale=10.0,  # ë„ˆë¬´ í° ê°’
       temporal_weight_range=(0.01, 1.0)  # ë„ˆë¬´ ê·¹ë‹¨ì 
   )
   
   # ì˜¬ë°”ë¥¸ ì˜ˆì‹œ
   loss_fn = Priority1_HuberMultiCriteriaLoss(
       gradient_weight_scale=2.0,   # ì ì ˆí•œ ê°’
       temporal_weight_range=(0.3, 1.0)  # ê· í˜•ì¡íŒ ë²”ìœ„
   )
   ```

2. **ê°€ì¤‘ì¹˜ ì •ê·œí™” ë¬´ì‹œ**
   - ëŒ€ë¶€ë¶„ì˜ ì†ì‹¤í•¨ìˆ˜ì—ëŠ” ë‚´ì¥ ì •ê·œí™”ê°€ ìˆì§€ë§Œ, ê·¹ë‹¨ì ì¸ íŒŒë¼ë¯¸í„° ì„¤ì • ì‹œ ì£¼ì˜
   - ê°€ì¤‘ì¹˜ì˜ ë²”ìœ„ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  clipping í™•ì¸

3. **ê²€ì¦ ë°ì´í„° í¸í–¥**
   - ì´ìƒì¹˜ì™€ ê¸‰ë³€ì´ í¬í•¨ëœ ëŒ€í‘œì ì¸ ê²€ì¦ ì„¸íŠ¸ êµ¬ì„±
   - ì‹œê°„ì  ë¶„í•  ì‹œ ë¯¸ë˜ ì‹œì  í¬í•¨ í™•ì¸

#### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ë°°ì¹˜ í¬ê¸° ì¡°ì •**
   ```python
   # ê°€ì¤‘ì¹˜ ê³„ì‚°ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ê³ ë ¤
   # ê¸°ì¡´ ë°°ì¹˜ í¬ê¸°ì˜ 70-80% ê¶Œì¥
   ```

2. **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**
   ```python
   # ê°€ì¤‘ì¹˜ë¡œ ì¸í•œ ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€
   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
   ```

3. **í•™ìŠµë¥  ì¡°ì •**
   ```python
   # ê°€ì¤‘ì¹˜ ì†ì‹¤í•¨ìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ë‚®ì€ í•™ìŠµë¥  í•„ìš”
   optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # ê¸°ì¡´ì˜ 50-70%
   ```

### ğŸ“š ì¶”ê°€ ìë£Œ ë° ì°¸ê³ ë¬¸í—Œ

#### ê´€ë ¨ ë…¼ë¬¸
- Huber Loss: "Robust Estimation of a Location Parameter" (Huber, 1964)
- Quantile Regression: "Regression Quantiles" (Koenker & Bassett, 1978)
- Multi-task Learning: "A Survey on Multi-Task Learning" (Zhang & Yang, 2017)

#### êµ¬í˜„ ì°¸ê³ ìë£Œ
- PyTorch ê³µì‹ ë¬¸ì„œ: Loss Functions
- Scikit-learn: Robust Regression
- TensorFlow Probability: Quantile Regression

#### ì‘ìš© ë¶„ì•¼ë³„ ê°€ì´ë“œ
- **ê¸ˆìœµ ì‹œê³„ì—´**: Priority 2 (MAE Outlier-focused) ì¶”ì²œ
- **IoT ì„¼ì„œ ë°ì´í„°**: Priority 1 (Huber Multi-criteria) ì¶”ì²œ
- **ì˜ë£Œ ì‹ í˜¸**: Priority 5 (Quantile Loss) ì¶”ì²œ
- **ì œì¡°ì—… ëª¨ë‹ˆí„°ë§**: Priority 4 (Gradient-based) ì¶”ì²œ

---

## ê²°ë¡ 

ì´ 6ê°€ì§€ ìš°ì„ ìˆœìœ„ë³„ ì†ì‹¤í•¨ìˆ˜ëŠ” ì´ìƒì¹˜ì™€ ê¸‰ê²©í•œ ë³€í™”ê°€ ì¤‘ìš”í•˜ê³ , ë¯¸ë˜ ì‹œì ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ì‹œê³„ì—´ íšŒê·€ ì‘ì—…ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. 

**ì„ íƒ ê°€ì´ë“œë¼ì¸:**
- ğŸš€ **ë¹ ë¥¸ ì‹œì‘**: Priority 1 (Huber Multi-criteria)
- ğŸ¯ **ì´ìƒì¹˜ ì§‘ì¤‘**: Priority 2 (MAE Outlier-focused)  
- ğŸ¤– **ìë™í™”**: Priority 3 (Adaptive Weight)
- âš¡ **ê¸‰ë³€ ì§‘ì¤‘**: Priority 4 (Gradient-based)
- ğŸ“Š **ë¶ˆí™•ì‹¤ì„±**: Priority 5 (Quantile)
- ğŸ”¬ **í¬ê´„ì **: Priority 6 (Multi-task)

ê° ì†ì‹¤í•¨ìˆ˜ëŠ” íŠ¹ì • ìƒí™©ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì„ íƒí•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.